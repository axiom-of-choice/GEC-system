{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Challenge related with a GEC system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Table of contents:\n",
        "\n",
        "- [Challenge related with a GEC system](#challenge-related-with-a-gec-system)\n",
        "- [Explanation](#explanation)\n",
        "- [Just Setting Up](#just-setting-up)\n",
        "- [Dataset Download and parser](#dataset-downloader-and-parser)\n",
        "    - [Dataset Exploration and Parsing](#some-exploration-of-the-data-and-the-parsing)\n",
        "- [Preprocessing for T5 Model](#t5-preprocessor)\n",
        "- [Training the T5 Model](#trainer-class)\n",
        "- [Inference Engines](#inference-engines)\n",
        "    - [Base Inference Engine](#base-inference-engine-and-helper)\n",
        "    - [T5 Inference Engine](#t5-inference-engine)\n",
        "    - [Llama 3 Inference Engine](#llama-3-inference-engine)\n",
        "- [Evaluation](#evaluation)\n",
        "    - [Evaluation helpers](#evaluator-class)\n",
        "    - [Evaluation of the FCE test data with exact match and GLEU](#evaluating-fce-test-data)\n",
        "        - [T5](#evaluating-the-test-dataset-fce-with-t5-using-gleu-and-exact-match)\n",
        "        - [LLAMA3](#evaluating-test-data-fce-with-llama-3-using-gleu-and-exact-match)\n",
        "    - [Evaluation of Medical data with exact match and GLEU](#evaluating-our-medical-data)\n",
        "        - [T5](#evaluating-with-t5-using-exact-match-and-gleu)\n",
        "        - [LLAMA](#evaluating-with-llama3-using-exact-match-and-gleu)\n",
        "    - [Evaluation of ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Just Setting Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfH9JRQoy-Nh"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers tqdm scikit-learn sentencepiece torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kt1Eh9fX4G9z"
      },
      "outputs": [],
      "source": [
        "## Imports\n",
        "\n",
        "import os\n",
        "# import openai\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Tuple\n",
        "import os\n",
        "import tarfile\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "77f0SKp44CoF"
      },
      "outputs": [],
      "source": [
        "# Create logger\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Remove all handlers associated with the root logger object (avoid duplicate logs)\n",
        "for handler in logger.handlers[:]:\n",
        "    logger.removeHandler(handler)\n",
        "\n",
        "# Create handler that outputs to notebook cell\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(funcName)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "logger.addHandler(handler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a03Tbnn20WGE"
      },
      "outputs": [],
      "source": [
        "# constants, ideally defined in other file and imported if done in a github repo\n",
        "\n",
        "FCE_URL = \"https://www.cl.cam.ac.uk/research/nl/bea2019st/data/fce_v2.1.bea19.tar.gz\"\n",
        "FCE_DEV_NAME = \"fce.dev.gold.bea19.m2\"\n",
        "FCE_TRAIN_NAME = \"fce.train.gold.bea19.m2\"\n",
        "FCE_TEST_NAME = \"fce.test.gold.bea19.m2\"\n",
        "FCE_DOWNLOAD_DATASET_DIR = os.path.join(os.getcwd(), \"data\")\n",
        "FCE_DATASET_DIR = os.path.join(os.getcwd(), \"data/fce/m2\")\n",
        "SENTENCE_TAG = \"S \"\n",
        "ANNOTATION_TAG = \"A \"\n",
        "NO_EDIT_TAG = \"noop\"\n",
        "SEP = \"|||\"\n",
        "## We're gonna use T5 small for this task because we're just correcting spelling and we don't wait to wait hours for training.\n",
        "MODEL_NAME = \"t5-small\"\n",
        "FINETUNED_MODEL_OUTPUT_DIR = \"./t5_finetuned\"\n",
        "GENERAL_PROMPT_PATH = os.path.join(os.getcwd(), \"config/prompt_general.txt\")\n",
        "LLAMA3_ENDPOINT = \"http://127.0.0.1:11434/api/generate\"\n",
        "TEXT_TO_REPLACE_IN_PROMPT = \"<text_to_replace>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Downloader and Parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This class orchrestrates the download of the BEA19 files and parsing them to store them for reusability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "er9tr-c83u1w"
      },
      "outputs": [],
      "source": [
        "### Dataset Loader class handling the dataset creation\n",
        "import tarfile\n",
        "from datasets import Dataset, DatasetDict\n",
        "import requests\n",
        "from typing import Optional, List, Dict\n",
        "\n",
        "\n",
        "class M2DatasetLoader:\n",
        "    \"\"\"\n",
        "    Loader for datasets in M2 format. Right now it's only tested for BCE2019\n",
        "\n",
        "    Args:\n",
        "        dataset_dir (str): Directory where the dataset is stored. Default is FCE_DATASET_DIR.\n",
        "        train_file (str): Name of the training file. Default is FCE_TRAIN_NAME.\n",
        "        dev_file (str): Name of the development file. Default is FCE_DEV_NAME.\n",
        "        test_file (str): Name of the test file. Default is FCE_TEST_NAME.\n",
        "        dataset_url (str): URL to download the dataset from. Default is FCE_URL.\n",
        "        fce_download_dir (str): Directory to download the dataset to. Default is FCE_DOWNLOAD_DATASET_DIR.\n",
        "    \"\"\"\n",
        "    def __init__(self,dataset_dir: str = FCE_DATASET_DIR, train_file: str = FCE_TRAIN_NAME,\n",
        "                 dev_file: str = FCE_DEV_NAME, test_file: str = FCE_TEST_NAME, dataset_url: str = FCE_URL,\n",
        "                 fce_download_dir: str = FCE_DOWNLOAD_DATASET_DIR):\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.fce_download_dir = fce_download_dir\n",
        "        self.train_file = os.path.join(dataset_dir, train_file)\n",
        "        self.dev_file = os.path.join(dataset_dir, dev_file)\n",
        "        self.test_file = os.path.join(dataset_dir, test_file)\n",
        "        self.url = dataset_url\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.dataset = None\n",
        "\n",
        "    def download_and_extract(self) -> None:\n",
        "        \"\"\"\n",
        "        Download the file and extract it into a directory if it does not exist\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.fce_download_dir):\n",
        "            os.makedirs(self.fce_download_dir, exist_ok=True)\n",
        "            self.logger.info(f\"Downloading BEA 2019 dataset...\")\n",
        "            tar_path = os.path.join(self.fce_download_dir, \"bea19.tar.gz\")\n",
        "            with requests.get(self.url, stream=True) as r:\n",
        "                with open(tar_path, 'wb') as f:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "            self.logger.info(\"Extracting dataset...\")\n",
        "            with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "                tar.extractall(path=self.fce_download_dir)\n",
        "            os.remove(tar_path)\n",
        "            self.logger.info(\"BEA dataset downloaded and extracted successfully.\")\n",
        "        else:\n",
        "            self.logger.warning(\"BEA dataset already exists locally.\")\n",
        "\n",
        "    def load_dataset(self) -> DatasetDict:\n",
        "        \"\"\" Create the dataset in the Transformers format \"\"\"\n",
        "        dataset = DatasetDict({\n",
        "            'train': Dataset.from_list(self._parse_m2_file(self.train_file)),\n",
        "            'validation': Dataset.from_list(self._parse_m2_file(self.dev_file)),\n",
        "            'test': Dataset.from_list(self._parse_m2_file(self.test_file))\n",
        "        })\n",
        "        self.logger.info(f\"Loaded BEA dataset: {len(dataset['train'])} train, {len(dataset['validation'])} dev, {len(dataset['test'])} test\")\n",
        "        self.dataset = dataset\n",
        "        return dataset\n",
        "    \n",
        "    def save_dataset(self, output_dir: str = os.path.join(os.getcwd(), FCE_DOWNLOAD_DATASET_DIR)) -> None:\n",
        "        \"\"\"\n",
        "        Parses the M2 files and saves the resulting DatasetDict to disk in HuggingFace format.\n",
        "        Args:\n",
        "            output_dir (str): Directory to save the dataset.\n",
        "        \"\"\"\n",
        "        if self.dataset is None:\n",
        "            self.dataset = self.load_dataset()\n",
        "        self.dataset.save_to_disk(output_dir + \"parsed_fce_dataset\")\n",
        "        self.logger.info(f\"Saved parsed dataset to {output_dir}\")\n",
        "\n",
        "    def _parse_m2_file(self, filepath: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Parses an M2 file and returns a list of {source, target} dictionaries,\n",
        "        where each 'target' corresponds to one annotator's corrections.\n",
        "        \"\"\"\n",
        "        data = []\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        sentence = \"\"\n",
        "        edits_by_annotator = dict()\n",
        "\n",
        "        for line in lines + ['\\n']:  # Add sentinel newline\n",
        "            line = line.strip()\n",
        "            if line.startswith(SENTENCE_TAG):\n",
        "                if sentence:\n",
        "                    if edits_by_annotator:\n",
        "                        for annotator_id, edits in edits_by_annotator.items():\n",
        "                            corrected = self._apply_m2_edits(sentence, edits)\n",
        "                            data.append({'source': sentence, 'target': corrected})\n",
        "                    else:\n",
        "                        data.append({'source': sentence, 'target': sentence})\n",
        "                sentence = line[2:]\n",
        "                edits_by_annotator = dict()\n",
        "            elif line.startswith(ANNOTATION_TAG):\n",
        "                parts = line[2:].split(SEP)\n",
        "                span = list(map(int, parts[0].split()))\n",
        "                error_type = parts[1]\n",
        "                correction = parts[2]\n",
        "                annotator_id = int(parts[-1])\n",
        "                if annotator_id not in edits_by_annotator:\n",
        "                    edits_by_annotator[annotator_id] = []\n",
        "                edits_by_annotator[annotator_id].append((span, correction, error_type))\n",
        "            elif line == \"\" and sentence:\n",
        "                if edits_by_annotator:\n",
        "                    for annotator_id, edits in edits_by_annotator.items():\n",
        "                        corrected = self._apply_m2_edits(sentence, edits)\n",
        "                        data.append({'source': sentence, 'target': corrected})\n",
        "                else:\n",
        "                    data.append({'source': sentence, 'target': sentence})\n",
        "                sentence = \"\"\n",
        "                edits_by_annotator = dict()\n",
        "        return data\n",
        "\n",
        "    def _apply_m2_edits(self, sentence: str, edits: list):\n",
        "        \"\"\"\n",
        "        Applies M2 format edits to the original sentence.\n",
        "        :param sentence: Original sentence (string)\n",
        "        :param edits: List of (span, correction, error_type)\n",
        "        :return: Corrected sentence\n",
        "        \"\"\"\n",
        "        tokens = sentence.strip().split()\n",
        "        offset = 0\n",
        "        for (span, correction, error_type) in edits:\n",
        "            start, end = span\n",
        "            if error_type == NO_EDIT_TAG or (start == -1 and end == -1):\n",
        "                continue\n",
        "            # Adjust indices by current offset\n",
        "            start_adj = start + offset\n",
        "            end_adj = end + offset\n",
        "            correction_tokens = correction.strip().split() if correction.strip() else []\n",
        "            tokens = tokens[:start_adj] + correction_tokens + tokens[end_adj:]\n",
        "            offset += len(correction_tokens) - (end - start)\n",
        "        return ' '.join(tokens)\n",
        "    \n",
        "    @staticmethod\n",
        "    def most_common_edit_types(gold_m2_path: str, n: int = 10) -> List[Tuple[str, int]]:\n",
        "        \"\"\"\n",
        "        Returns the most common edit types in the gold M2 file.\n",
        "        :param gold_m2_path: Path to the gold M2 file.\n",
        "        :param n: Number of most common edit types to return.\n",
        "        :return: List of tuples (edit_type, count).\n",
        "        \"\"\"\n",
        "        from collections import Counter\n",
        "        edit_types = Counter()\n",
        "        \n",
        "        with open(gold_m2_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.startswith(ANNOTATION_TAG):\n",
        "                    parts = line[2:].split(SEP)\n",
        "                    error_type = parts[1]\n",
        "                    edit_types[error_type] += 1\n",
        "        \n",
        "        return edit_types.most_common(n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86oVxDVb4V9G",
        "outputId": "5fff7ede-983a-4a29-bbb4-9b472e6b455c"
      },
      "outputs": [],
      "source": [
        "loader = M2DatasetLoader()\n",
        "\n",
        "loader.download_and_extract()\n",
        "dataset = loader.load_dataset()\n",
        "loader.save_dataset(output_dir=os.getcwd() + \"/data/fce/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some exploration of the data and the parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-e2ElvC3XC9"
      },
      "source": [
        "The file has one or more line per sentence, and one or more line per annotation, grouped together by a blank line.\n",
        "If there si a *noop* tag the there is no edit.\n",
        "\n",
        "\n",
        "We need a set of functions to actually put this corrections into the sentence to build up a dataset with \"source\" and \"target\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 405,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwkgL_Wj267x",
        "outputId": "a0f38808-8de6-45b8-e109-e674b679ed82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S Dear Sir or Madam ,\n",
            "A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S I am writing in order to express my disappointment about your musical show \" Over the Rainbow \" .\n",
            "A 9 10|||R:PREP|||with|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S I saws the show 's advertisement hanging up of a wall in London where I was spending my holiday with some friends . I convinced them to go there with me because I had heard good references about your Company and , above all , about the main star , Danny Brook .\n",
            "A 1 2|||R:VERB:TENSE|||saw|||REQUIRED|||-NONE-|||0\n",
            "A 8 9|||R:PREP|||on|||REQUIRED|||-NONE-|||0\n",
            "A 36 37|||R:NOUN|||reviews|||REQUIRED|||-NONE-|||0\n",
            "A 37 38|||R:PREP|||of|||REQUIRED|||-NONE-|||0\n",
            "A 45 46|||R:PREP|||because of|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S The problems started in the box office , where we asked for the discounts you announced in the advertisement , and the man who was selling the tickets said that they did n't exist .\n",
            "A 3 4|||R:PREP|||at|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S Moreover , the show was delayed forty - five minutes and the worst of all was that Danny Brook had been replaced by another actor .\n",
            "A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S On the other hand , the theatre restaurant was closed because unknown reasons .\n",
            "A 0 4|||R:OTHER|||In addition|||REQUIRED|||-NONE-|||0\n",
            "A 10 11|||R:PREP|||for|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S You promised a perfect evening but it became a big disastrous !\n",
            "A 10 11|||R:MORPH|||disaster|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S I would like some kind of explanation and receive my\n"
          ]
        }
      ],
      "source": [
        "# Need yo have the data downloaded to explore it. Will be automatized in the next class\n",
        "train_data = os.path.join(FCE_DATASET_DIR, FCE_TRAIN_NAME)\n",
        "\n",
        "\n",
        "with open(train_data, 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "print(data[:1500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3w3_zOY6O_M"
      },
      "source": [
        "To see what the parser is doing let's take two examples. One without edit and one with some edits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fHDKvJV6VUl"
      },
      "source": [
        "Without edits: (First record o the file actually)\n",
        "\n",
        "```S Dear Sir or Madam ,\n",
        "A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 406,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeGexy6k6laV",
        "outputId": "40239183-2598-4565-885f-9bb2f0f633c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrong\n",
            "Dear Sir or Madam ,\n",
            "Corrected\n",
            "Dear Sir or Madam ,\n"
          ]
        }
      ],
      "source": [
        "print(\"Wrong\")\n",
        "print(dataset['train'][0].get(\"source\"))\n",
        "print(\"Corrected\")\n",
        "print(dataset['train'][0].get(\"target\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3ZGWjXS8Fc9"
      },
      "source": [
        "Many edits, like third record of the file:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0QTaTBg8PeL"
      },
      "source": [
        "\"I saws the show 's advertisement hanging up of a wall in London where I was spending my holiday with some friends .\n",
        "I convinced them to go there with me because I had heard good references about your Company and , above all , about the main star , Danny Brook .\"\n",
        "\n",
        "```\n",
        "A 1 2|||R:VERB:TENSE|||saw|||REQUIRED|||-NONE-|||0\n",
        "A 8 9|||R:PREP|||on|||REQUIRED|||-NONE-|||0\n",
        "A 36 37|||R:NOUN|||reviews|||REQUIRED|||-NONE-|||0\n",
        "A 37 38|||R:PREP|||of|||REQUIRED|||-NONE-|||0\n",
        "A 45 46|||R:PREP|||because of|||REQUIRED|||-NONE-|||0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKwiifh28TY9"
      },
      "source": [
        "Explanation:\n",
        "\n",
        "It bascially changes *saws* by *saw*, then *of* by *on*, *references* by *reviews*, again *about* by *of* and finally *about* by *because of*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 407,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiOF8Cvk0EfO",
        "outputId": "b356f2ed-98af-4579-a216-516fdd6216c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrong\n",
            "I saws the show 's advertisement hanging up of a wall in London where I was spending my holiday with some friends . I convinced them to go there with me because I had heard good references about your Company and , above all , about the main star , Danny Brook .\n",
            "Corrected\n",
            "I saw the show 's advertisement hanging up on a wall in London where I was spending my holiday with some friends . I convinced them to go there with me because I had heard good reviews of your Company and , above all , because of the main star , Danny Brook .\n"
          ]
        }
      ],
      "source": [
        "print(\"Wrong\")\n",
        "print(dataset['train'][2].get(\"source\"))\n",
        "print(\"Corrected\")\n",
        "print(dataset['train'][2].get(\"target\"))\n",
        "#print(dataset['validation'][0])\n",
        "#print(dataset['test'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA8_NAwl9t6x"
      },
      "source": [
        "Lets see another example where we delete something (I looked for it in the file)\n",
        "\n",
        "*consequently* needs to get deleted\n",
        "\n",
        "```\n",
        "S If you do n't agree , I will act consequently .\n",
        "A 9 10|||U:ADV||||||REQUIRED|||-NONE-|||0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 408,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAV7qrVh9z4c",
        "outputId": "07a5f6c8-01f0-4c1a-adc4-8d492d76198b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrong\n",
            "If you do n't agree , I will act consequently .\n",
            "Corrected\n",
            "If you do n't agree , I will act .\n"
          ]
        }
      ],
      "source": [
        "print(\"Wrong\")\n",
        "print(dataset['train'][8].get(\"source\"))\n",
        "print(\"Corrected\")\n",
        "print(dataset['train'][8].get(\"target\"))\n",
        "#print(dataset['validation'][0])\n",
        "#print(dataset['test'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9j1fyo_-Opw"
      },
      "source": [
        "With punctuation\n",
        "\n",
        "In this case, we need to add a comma and a quoute\n",
        "\n",
        "```\n",
        "\n",
        "S She began to read \" Dear Carolin ..\n",
        "A 4 4|||M:PUNCT|||,|||REQUIRED|||-NONE-|||0\n",
        "A 8 8|||M:PUNCT|||\"|||REQUIRED|||-NONE-|||0\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7kORX_c-a5O",
        "outputId": "0a03d64d-1675-4fa3-b676-5178dfe95acc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrong\n",
            "She began to read \" Dear Carolin ..\n",
            "Corrected\n",
            "She began to read , \" Dear Carolin \" ..\n"
          ]
        }
      ],
      "source": [
        "print(\"Wrong\")\n",
        "print(dataset['train'][15].get(\"source\"))\n",
        "print(\"Corrected\")\n",
        "print(dataset['train'][15].get(\"target\"))\n",
        "#print(dataset['validation'][0])\n",
        "#print(dataset['test'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 444,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'There are eye - max cinema , museums , garelty .',\n",
              " 'target': 'There are eye - max cinema , museums , a gallery .'}"
            ]
          },
          "execution_count": 444,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_from_disk\n",
        "parsed_dataset = load_from_disk(os.path.join(os.getcwd(), \"data/fce/parsed_fce_dataset\"))\n",
        "\n",
        "parsed_dataset['test'][1266]  # Check the first example in the parsed dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk2nkDWTADU7"
      },
      "source": [
        "Now that we already have a parser and loader of the dataset for our needs using M2 file, we will create a simple preprocessor for the T5 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T5 PreProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Postprocessor class that handles tokenization for the data used by T5 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Osxngv6n3X-z"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizer\n",
        "from datasets import DatasetDict\n",
        "from typing import Dict\n",
        "\n",
        "class T5Preprocessor:\n",
        "    \"\"\"\n",
        "    T5 Preprocessor for the data. Could be implemented as abstract class to have multiple preprocessors.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (PreTrainedTokenizer): The tokenizer to use.\n",
        "        max_lenght (int): The maximum lenght of the input. Default is 512.\n",
        "        truncation (bool): Whether to truncate the input. Default is True.\n",
        "        padding (str): The padding to use. Default is \"max_length\".\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, truncation: bool = True, padding: str = \"max_length\"):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.truncation = truncation\n",
        "        self.padding = padding\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def _preprocess_function(self, examples: Dict[str,str], max_length: int) -> Dict[str, List[List[int]]]:\n",
        "        \"\"\"\n",
        "        Preprocess examples tokenizing them using the instance parameters.\n",
        "        \"\"\"\n",
        "        inputs = [\"correct grammar: \" + s for s in examples['source']]\n",
        "        targets = [t for t in examples['target']]\n",
        "        model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=self.truncation, padding=self.padding)\n",
        "        labels = self.tokenizer(targets, max_length=max_length, truncation=self.truncation, padding=self.padding)\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "    \n",
        "    @staticmethod\n",
        "    def _get_max_input_length(dataset: DatasetDict) -> int:\n",
        "        \"\"\"\n",
        "        Get the maximum input length in the dataset.\n",
        "        \"\"\"\n",
        "        max_length = 0\n",
        "        for split in dataset:\n",
        "            split_max = max(len(x) for x in dataset[split]['source'])\n",
        "            #self.logger.info(f\"Max input_ids length in {split}: {split_max}\")\n",
        "            max_length = max(max_length, split_max)\n",
        "            #self.logger.info(f\"Overall max input_ids length: {max_length}\")\n",
        "        return max_length\n",
        "    \n",
        "    \n",
        "    def save_tokenized_dataset(self, dataset: DatasetDict, output_dir: str) -> None:\n",
        "        \"\"\"\n",
        "        Save the tokenized dataset to disk.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Saving tokenized dataset to {output_dir}...\")\n",
        "        dataset.save_to_disk(output_dir)\n",
        "        self.logger.info(\"Tokenized dataset saved successfully.\")\n",
        "\n",
        "    def preprocess(self, dataset: DatasetDict, max_length: int) -> DatasetDict:\n",
        "        \"\"\"\n",
        "        Preprocess the dataset.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Preprocessing dataset...\")\n",
        "        tokenized_dataset = dataset.map(lambda examples: self._preprocess_function(examples, max_length), batched=True)\n",
        "        self.logger.info(\"Dataset preprocessed\")\n",
        "        return tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147,
          "referenced_widgets": [
            "ce276d79bf73439c963e79b02c767794",
            "ff40b7f85c3a4ec29b2493d6a96b8553",
            "bdc85929710b4c0a8bc404f845458970",
            "6c7ed26dff004f979e336abb19029e1f",
            "4195890956a7418da1a88f32677e7405",
            "97bc4ab3288e43448c1d4670156adee1",
            "899e8ba560184b1a8228a7df1b27c994",
            "43fa86bc7caf4bd1a4e06ace7e0b25b2",
            "ff69fbbae64a40218adf260529fdb5b4",
            "23e48d091d584996b6baf9d3534447cd",
            "31ea86456fd14d2889100383253460b1",
            "3e5f706edd4f40b89c51070623366d55",
            "5e472c8b6ea8481a91ae73d55aa652b3",
            "cd95e5d0f489478f94ecc09980f12e73",
            "055c7fcf598745cf99315e80e81709c0",
            "216b495b32934706b944cf39c4b22482",
            "3b96becefb6e49af8b3413b929ba5cc0",
            "315a9ddbc2284762b1e80bd0a92a0b4d",
            "ae683a19f8df4c5c89ecd2d1373d3c66",
            "3ec7a5e8f6b849f8bab515ee5b93af35",
            "ef8d3a0680b74c6e9df24794017ea12e",
            "279659dc07444734b085c8169f867b70",
            "08a6b7839b764f4fb776aea3af5acf78",
            "2297133da4464284ab9d40d2f6ced178",
            "74498800f98e4fff8f6bb8071dc6d50d",
            "72ef81b8e8a54eefa18b02b44f4874a3",
            "fa5e46db179b4fe5a9da60eca0ebe021",
            "f9bf949307354e12a70c3da2292058ed",
            "48adf339642c40e09940e3c7365fb773",
            "2f5637f5aa8c4e6aadf6220867f5f6b0",
            "45a9de88d3114a5e84c3838dfe93585f",
            "fdf4c595694342fd87d9b758ff52f087",
            "e92402f5608045199acb7a0e8983f9c3"
          ]
        },
        "id": "Yoj7GoEAGFbh",
        "outputId": "2768d5be-3f00-4f2f-826c-da2f70a4ec46"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer\n",
        "from datasets import load_from_disk\n",
        "\n",
        "dataset = load_from_disk(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce/parsed_fce_dataset\"))\n",
        "max_length = T5Preprocessor._get_max_input_length(dataset)  # This will give you the max input length in the dataset for saving the preprocessed dataset to train the model\n",
        "\n",
        "preprocessor = T5Preprocessor(tokenizer=T5Tokenizer.from_pretrained(MODEL_NAME))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9zU9eBeGfO5",
        "outputId": "f120a7cc-60eb-4762-8903-6a7d629c7573"
      },
      "outputs": [],
      "source": [
        "preprocessed_dataset = preprocessor.preprocess(dataset, max_length=max_length)\n",
        "preprocessed_dataset.save_to_disk(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"/fce/preprocessed_fce_dataset\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFnnTmlWGAxT"
      },
      "source": [
        "Now what about the trainig class?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trainer class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Class created to handle training in a repeatable way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9ZPi0RWOGDej"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Optional\n",
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from datasets import DatasetDict\n",
        "import torch\n",
        "\n",
        "\n",
        "class T5Trainer:\n",
        "    def __init__(self, model_name: str = MODEL_NAME, output_dir: str = FINETUNED_MODEL_OUTPUT_DIR,\n",
        "                 logging_dir: str = \"./finetune_logs\", save_strategy: str = \"epoch\", resume_from_dir: Optional[str] = None,\n",
        "                 batch_size: int = 8, mixed_precision: bool = False, early_stopping: bool = True,\n",
        "                 eval_strategy: str = \"epoch\", early_stopping_patience: int = 3, early_stopping_threshold: float = 0.0, \n",
        "                 metric_for_best_model: str = \"eval_loss\", greater_is_better: bool = False):\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.output_dir = output_dir\n",
        "        self.logging_dir = logging_dir\n",
        "        self.save_strategy = save_strategy\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.resume_from_dir = resume_from_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.mixed_precision = mixed_precision\n",
        "        self.early_stopping = early_stopping\n",
        "        self.eval_strategy = eval_strategy\n",
        "        self.early_stopping_patience = early_stopping_patience\n",
        "        self.early_stopping_threshold = early_stopping_threshold\n",
        "        self.metric_for_best_model = metric_for_best_model\n",
        "        self.greater_is_better = greater_is_better\n",
        "        \n",
        "\n",
        "    def _check_if_gpu_available(self) -> bool:\n",
        "        \"\"\"\n",
        "        Check if GPU is available.\n",
        "        \"\"\"\n",
        "        available = torch.cuda.is_available()\n",
        "\n",
        "        if available:\n",
        "            self.logger.info(\"GPU available\")\n",
        "        else:\n",
        "            self.logger.warning(\"GPU not available\")\n",
        "\n",
        "        return available\n",
        "\n",
        "    def _check_if_model_already_available(self) -> bool:\n",
        "        \"\"\"\n",
        "        Check if model is already available.\n",
        "        \"\"\"\n",
        "        available = os.path.exists(self.output_dir)\n",
        "\n",
        "        if available:\n",
        "            self.logger.info(\"Model already available\")\n",
        "        else:\n",
        "            self.logger.warning(\"Model not available\")\n",
        "\n",
        "        return available\n",
        "\n",
        "    def _create_unique_dir_for_model(self) -> str:\n",
        "\n",
        "        \"\"\"\n",
        "        Create a unique directory for the model based on the current time.\n",
        "        \"\"\"\n",
        "        current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        unique_dir = os.path.join(self.output_dir, current_time)\n",
        "        os.makedirs(unique_dir, exist_ok=True)\n",
        "        self.logger.info(f\"Created unique directory for model: {unique_dir}\")\n",
        "        return unique_dir\n",
        "\n",
        "\n",
        "    def train(self, tokenized_dataset:DatasetDict, epochs=3, learning_rate: float = 3e-4, eval_strategy = \"epoch\") -> Tuple[str, T5ForConditionalGeneration, T5Tokenizer]:\n",
        "        \"\"\"\n",
        "        Train the T5 model.\n",
        "        Args:\n",
        "            tokenized_dataset (DatasetDict): The tokenized dataset.\n",
        "            epochs (int): The number of epochs to train for.\n",
        "        Returns:\n",
        "            Tuple[Model_Dir (ID), T5ForConditionalGeneration, T5Tokenizer]: The trained model and tokenizer.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Checking if GPU is available...\")\n",
        "        self._check_if_gpu_available()\n",
        "        #self.logger.info(\"Checking if model is already available...\")\n",
        "        #self._check_if_model_already_available()\n",
        "        # Determine the output directory based on whether we're resuming or starting fresh\n",
        "        if self.resume_from_dir and os.path.exists(self.resume_from_dir):\n",
        "            self.output_dir = self.resume_from_dir\n",
        "            self.logger.info(f\"Resuming training from directory: {self.output_dir}\")\n",
        "        else:\n",
        "             self.output_dir = self._create_unique_dir_for_model()\n",
        "             self.logger.info(f\"Starting new training in directory: {self.output_dir}\")\n",
        "\n",
        "        # self.output_dir = self._create_unique_dir_for_model()\n",
        "\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.output_dir,\n",
        "            per_device_train_batch_size=self.batch_size,\n",
        "            num_train_epochs=epochs,\n",
        "            eval_strategy = self.eval_strategy,\n",
        "            save_strategy=self.save_strategy,\n",
        "            logging_dir=self.logging_dir,\n",
        "            learning_rate=learning_rate,\n",
        "            report_to=\"none\", #Needed to avoid wandb api key request.,\n",
        "            # fp16=self.mixed_precision,  # Enable mixed precision training if specified\n",
        "        )\n",
        "        \n",
        "        if self.early_stopping:\n",
        "            training_args.load_best_model_at_end = True\n",
        "            training_args.metric_for_best_model = self.metric_for_best_model\n",
        "            training_args.greater_is_better = self.greater_is_better\n",
        "            training_args.save_total_limit = 1\n",
        "            training_args.eval_strategy = self.eval_strategy\n",
        "        self.logger.info(\"Training arguments set\")\n",
        "        \n",
        "        if self.mixed_precision:\n",
        "            training_args.fp16 = True\n",
        "            self.logger.info(\"Mixed precision training enabled\")\n",
        "\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset['train'],\n",
        "            eval_dataset=tokenized_dataset['validation']\n",
        "        )\n",
        "        \n",
        "        if self.early_stopping:\n",
        "            trainer.add_callback(EarlyStoppingCallback(self.early_stopping_patience, self.early_stopping_threshold))\n",
        "            self.logger.info(\"Early stopping callback added\")\n",
        "\n",
        "          # Resume training if a checkpoint exists in the output directory\n",
        "        if self.resume_from_dir and os.path.exists(self.resume_from_dir):\n",
        "             # The Trainer class automatically handles resuming from a directory if it exists\n",
        "             # and contains a checkpoint. You don't need to explicitly load the state dicts\n",
        "             # if you are using the Trainer's resume functionality.\n",
        "             trainer.train(resume_from_checkpoint=self.output_dir)\n",
        "        else:\n",
        "            trainer.train()\n",
        "\n",
        "\n",
        "        self.logger.info(\"T5 model trained\")\n",
        "        trainer.save_model(self.output_dir)\n",
        "        self.model.save_pretrained(self.output_dir)\n",
        "        self.tokenizer.save_pretrained(self.output_dir)\n",
        "        return self.output_dir, self.model, self.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "preprocessed_dataset = load_from_disk(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"preprocessed_fce_dataset\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "E0OHnY7bHgwl",
        "outputId": "2585a7c2-5200-4e4c-ed1d-c8bc54733194"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#trainer = T5Trainer(resume_from_dir=\"./t5_finetuned/20250530-172532\")\n",
        "# trainer = T5Trainer(batch_size=16, resume_from_dir=\"/Users/isaac/Developer/sample/t5_finetuned/20250530-121239/checkpoint-591\")  # Enable mixed precision training\n",
        "\n",
        "trainer = T5Trainer(batch_size=8)\n",
        "\n",
        "model_dir, model, tokenizer = trainer.train(preprocessed_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference Engines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base Inference Engine and Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "class BaseInferenceEngine(ABC):\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "    @abstractmethod\n",
        "    def correct_sentence(self, sentence: str) -> str:\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def batch_correct(self, sentences: list, batch_size: int = 16) -> list:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## T5 Inference Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inference engines to orchestrate model loading, and implements methods for single and batch inference just calling a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, Union, Optional\n",
        "\n",
        "\n",
        "class T5InferenceEngine(BaseInferenceEngine):\n",
        "    def __init__(self, model_dir: str, max_length: Optional[int] = None):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.t5_model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
        "        self.max_length = max_length if max_length is not None else self.tokenizer.model_max_length\n",
        "        self.logger.info(f\"T5 model loaded from {model_dir} with max length {self.max_length}\")\n",
        "        \n",
        "\n",
        "    def correct_sentence(self, sentence:str) -> str:\n",
        "        \"\"\"\n",
        "        Corrects a sentence using the T5 model.\n",
        "        Args:\n",
        "            sentence (str): The sentence to correct.\n",
        "        Returns:\n",
        "            str: The corrected sentence.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Correcting sentence: {sentence}\")\n",
        "        inputs = self.tokenizer(\"correct grammar: \" + sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length)\n",
        "        outputs = self.t5_model.generate(**inputs, max_length=self.max_length)\n",
        "        corrected_sentence = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        self.logger.info(f\"Corrected sentence: {corrected_sentence}\")\n",
        "        return corrected_sentence\n",
        "    \n",
        "    def batch_correct(self, sentences: list, batch_size: int = 16) -> list:\n",
        "        \"\"\"\n",
        "        Corrects a batch of sentences using the T5 model.\n",
        "        Args:\n",
        "            sentences (list): List of sentences to correct.\n",
        "            batch_size (int): Number of sentences per batch.\n",
        "        Returns:\n",
        "            list: List of corrected sentences.\n",
        "        \"\"\"\n",
        "        corrected = []\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            self.logger.info(f\"Processing batch {i // batch_size + 1} with size {min(batch_size, len(sentences) - i)}\")\n",
        "            batch = sentences[i:i+batch_size]\n",
        "            inputs = self.tokenizer(\n",
        "                [\"correct grammar: \" + s for s in batch],\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=self.max_length\n",
        "            )\n",
        "            outputs = self.t5_model.generate(**inputs, max_length=self.max_length)\n",
        "            batch_corrected = [self.tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
        "            corrected.extend(batch_corrected)\n",
        "        self.logger.info(f\"Batch correction completed. Total corrected sentences: {len(corrected)}\")\n",
        "        return corrected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "650"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dir = \"/Users/isaac/Developer/GEC-system/models/improved\"\n",
        "t5_inference_engine = T5InferenceEngine(model_dir=model_dir, max_length=650)\n",
        "t5_inference_engine.max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 14:20:44,073 - INFO - T5InferenceEngine - correct_sentence - Correcting sentence: You is a apple.\n",
            "2025-06-01 14:20:44,397 - INFO - T5InferenceEngine - correct_sentence - Corrected sentence: You are an apple.\n",
            "Wrong sentence: You is a apple.\n",
            "Corrected sentence: You are an apple.\n"
          ]
        }
      ],
      "source": [
        "wrong_sentence = \"You is a apple.\"\n",
        "theorycally_corrected_sentence = \"I have an apple.\"\n",
        "\n",
        "\n",
        "corrected_sentence = t5_inference_engine.correct_sentence(wrong_sentence)\n",
        "\n",
        "print(f\"Wrong sentence: {wrong_sentence}\")\n",
        "print(f\"Corrected sentence: {corrected_sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/isaac/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Llama 3 Inference Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See docs to run and query llama3 8b locally using ollama\n",
        "\n",
        "https://ollama.com/library/llama3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Union, Dict\n",
        "import json as _json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import requests\n",
        "\n",
        "class Llama3InferenceEngine(BaseInferenceEngine):\n",
        "    model_name: str = \"llama3\"\n",
        "    stream: bool = False\n",
        "    response_format: dict = {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"original_text\": {\"type\": \"string\"},\n",
        "            \"corrected_text\": {\"type\": \"string\"}\n",
        "        },\n",
        "        \"required\": [\"original_text\", \"corrected_text\"]\n",
        "    }\n",
        "    \n",
        "    options: Dict[str, Union[str, int, float]] = {\n",
        "        \"temperature\": 0.2,\n",
        "        \"top_k\": 20,\n",
        "        \"top_p\": 0.5,\n",
        "        \"seed\": 0\n",
        "    }\n",
        "    \n",
        "    \n",
        "    def __init__(self, model_endpoint: str, prompt_path: str):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.model_endpoint = model_endpoint\n",
        "        self.prompt_path = prompt_path\n",
        "        self.prompt = self._parse_prompt()\n",
        "        self.logger.info(f\"Llama3InferenceEngine initialized with model endpoint: {self.model_endpoint} and prompt path: {self.prompt_path}, options: {self.options}\")\n",
        "        \n",
        "    def _parse_prompt(self) -> str:\n",
        "        with open(self.prompt_path, 'r') as file:\n",
        "            prompt = file.read()\n",
        "        if not prompt:\n",
        "            self.logger.error(\"Prompt is empty. Please check the prompt file.\")\n",
        "            raise ValueError(\"Prompt is empty. Please check the prompt file.\")\n",
        "        return prompt\n",
        "    \n",
        "    def _replace_prompt_variables(self, sentence: str) -> str:\n",
        "        return self.prompt.replace(f\"{TEXT_TO_REPLACE_IN_PROMPT}\", sentence)\n",
        "\n",
        "    def send_correct_request(self, sentence: str) -> Dict[str, Union[str, Dict[str, str]]]:\n",
        "        prompt = self._replace_prompt_variables(sentence)\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                self.model_endpoint, \n",
        "                json={\n",
        "                    \"model\": self.model_name,\n",
        "                    \"prompt\": str(prompt),\n",
        "                    \"stream\": self.stream,\n",
        "                    \"format\": self.response_format,\n",
        "                    \"options\": self.options\n",
        "                }\n",
        "            )\n",
        "            response_data = response.json()\n",
        "            if not response_data:\n",
        "                self.logger.error(\"No corrected sentence returned from the model.\")\n",
        "                raise ValueError(\"No corrected sentence returned from the model.\")\n",
        "            if \"response\" in response_data and isinstance(response_data[\"response\"], str):\n",
        "                response_data[\"response\"] = _json.loads(response_data[\"response\"])\n",
        "            return response_data\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            self.logger.error(f\"Error during model inference: {e}\")\n",
        "            raise RuntimeError(f\"Error during model inference: {e}\")\n",
        "        \n",
        "    def correct_sentence(self, sentence: str) -> str:\n",
        "        self.logger.info(f\"Correcting sentence: {sentence}\")\n",
        "        response = self.send_correct_request(sentence)\n",
        "        if isinstance(response, dict):\n",
        "            corrected_sentence = response.get(\"response\", {}).get(\"corrected_text\", \"\")\n",
        "            self.logger.info(f\"Corrected sentence: {corrected_sentence}\")\n",
        "            return corrected_sentence\n",
        "        else:\n",
        "            return \"\"\n",
        "        \n",
        "    def batch_correct(self, sentences: list, batch_size: int = 16) -> list:\n",
        "        \"\"\"Not implemented for Llama3InferenceEngine as it does not support full batch inference yet.\n",
        "        This method is a placeholder to maintain interface consistency with BaseInferenceEngine.\n",
        "\n",
        "        Args:\n",
        "            sentences (list): _description_\n",
        "            batch_size (int, optional): _description_. Defaults to 16.\n",
        "\n",
        "        Returns:\n",
        "            list: _description_\n",
        "        \"\"\"\n",
        "        return super().batch_correct(sentences, batch_size)\n",
        "\n",
        "    # --- ASYNC BATCH INFERENCE ---\n",
        "    async def async_correct_sentence(self, session, sentence: str) -> str:\n",
        "        prompt = self._replace_prompt_variables(sentence)\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"prompt\": str(prompt),\n",
        "            \"stream\": self.stream,\n",
        "            \"format\": self.response_format,\n",
        "            \"options\": self.options\n",
        "        }\n",
        "        self.logger.info(f\"Sending async request for sentence: {sentence}\")\n",
        "        async with session.post(self.model_endpoint, json=payload) as resp:\n",
        "            response_data = await resp.json()\n",
        "            if not response_data:\n",
        "                return \"\"\n",
        "            if \"response\" in response_data and isinstance(response_data[\"response\"], str):\n",
        "                import json as _json\n",
        "                response_data[\"response\"] = _json.loads(response_data[\"response\"])\n",
        "                corrected_text = response_data.get(\"response\", {}).get(\"corrected_text\", \"\")\n",
        "                self.logger.info(f\"Async corrected sentence: {corrected_text}\")\n",
        "            \n",
        "                return corrected_text if corrected_text else \"\"\n",
        "            else:\n",
        "                self.logger.error(\"Response format is not as expected.\")\n",
        "                return \"\"\n",
        "\n",
        "    async def async_batch_correct(self, sentences, max_concurrent=5):\n",
        "        timeout = aiohttp.ClientTimeout(total=60)\n",
        "        semaphore = asyncio.Semaphore(max_concurrent)\n",
        "        async with aiohttp.ClientSession(timeout=timeout) as session:\n",
        "            async def sem_task(sentence):\n",
        "                async with semaphore:\n",
        "                    return await self.async_correct_sentence(session, sentence)\n",
        "            tasks = [sem_task(s) for s in sentences]\n",
        "            return await asyncio.gather(*tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 20:02:56,687 - INFO - Llama3InferenceEngine - __init__ - Llama3InferenceEngine initialized with model endpoint: http://127.0.0.1:11434/api/generate and prompt path: /Users/isaac/Developer/GEC-system/config/prompt_general.txt, options: {'temperature': 0.0, 'seed': 123, 'top_k': 10, 'top_p': 0.5}\n"
          ]
        }
      ],
      "source": [
        "llama3_engine = Llama3InferenceEngine(model_endpoint=LLAMA3_ENDPOINT, prompt_path=GENERAL_PROMPT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model': 'llama3',\n",
              " 'created_at': '2025-06-02T02:03:03.061195Z',\n",
              " 'response': {'original_text': 'Helo', 'corrected_text': 'Hello'},\n",
              " 'done': True,\n",
              " 'done_reason': 'stop',\n",
              " 'context': [128006,\n",
              "  882,\n",
              "  128007,\n",
              "  271,\n",
              "  2675,\n",
              "  527,\n",
              "  4689,\n",
              "  7580,\n",
              "  69225,\n",
              "  62172,\n",
              "  1493,\n",
              "  27358,\n",
              "  1887,\n",
              "  13,\n",
              "  4718,\n",
              "  3465,\n",
              "  374,\n",
              "  311,\n",
              "  1193,\n",
              "  5155,\n",
              "  1193,\n",
              "  32528,\n",
              "  11,\n",
              "  43529,\n",
              "  11,\n",
              "  477,\n",
              "  62603,\n",
              "  6103,\n",
              "  2768,\n",
              "  264,\n",
              "  480,\n",
              "  7650,\n",
              "  5410,\n",
              "  13,\n",
              "  3234,\n",
              "  539,\n",
              "  312,\n",
              "  28810,\n",
              "  11,\n",
              "  923,\n",
              "  11,\n",
              "  477,\n",
              "  4148,\n",
              "  2038,\n",
              "  13,\n",
              "  1442,\n",
              "  279,\n",
              "  11914,\n",
              "  374,\n",
              "  4495,\n",
              "  11,\n",
              "  471,\n",
              "  433,\n",
              "  35957,\n",
              "  627,\n",
              "  13622,\n",
              "  315,\n",
              "  279,\n",
              "  3288,\n",
              "  762,\n",
              "  4978,\n",
              "  1253,\n",
              "  539,\n",
              "  1205,\n",
              "  904,\n",
              "  4442,\n",
              "  11,\n",
              "  779,\n",
              "  499,\n",
              "  690,\n",
              "  471,\n",
              "  279,\n",
              "  11914,\n",
              "  35957,\n",
              "  304,\n",
              "  1455,\n",
              "  5157,\n",
              "  382,\n",
              "  791,\n",
              "  1455,\n",
              "  4279,\n",
              "  6103,\n",
              "  2997,\n",
              "  512,\n",
              "  12,\n",
              "  12362,\n",
              "  2500,\n",
              "  3492,\n",
              "  938,\n",
              "  7664,\n",
              "  2555,\n",
              "  198,\n",
              "  35970,\n",
              "  25,\n",
              "  358,\n",
              "  1097,\n",
              "  12703,\n",
              "  311,\n",
              "  1373,\n",
              "  922,\n",
              "  12131,\n",
              "  539,\n",
              "  1694,\n",
              "  520,\n",
              "  813,\n",
              "  1888,\n",
              "  627,\n",
              "  20523,\n",
              "  291,\n",
              "  25,\n",
              "  358,\n",
              "  1097,\n",
              "  12703,\n",
              "  311,\n",
              "  1373,\n",
              "  922,\n",
              "  12131,\n",
              "  539,\n",
              "  1694,\n",
              "  1664,\n",
              "  627,\n",
              "  35970,\n",
              "  512,\n",
              "  20523,\n",
              "  291,\n",
              "  512,\n",
              "  12,\n",
              "  82957,\n",
              "  452,\n",
              "  1656,\n",
              "  198,\n",
              "  35970,\n",
              "  25,\n",
              "  578,\n",
              "  7926,\n",
              "  374,\n",
              "  1120,\n",
              "  32249,\n",
              "  3201,\n",
              "  505,\n",
              "  279,\n",
              "  5740,\n",
              "  842,\n",
              "  627,\n",
              "  20523,\n",
              "  291,\n",
              "  25,\n",
              "  578,\n",
              "  7926,\n",
              "  374,\n",
              "  1120,\n",
              "  7504,\n",
              "  3201,\n",
              "  505,\n",
              "  279,\n",
              "  5740,\n",
              "  842,\n",
              "  627,\n",
              "  35970,\n",
              "  25,\n",
              "  2684,\n",
              "  527,\n",
              "  8071,\n",
              "  482,\n",
              "  1973,\n",
              "  34292,\n",
              "  11,\n",
              "  51677,\n",
              "  11,\n",
              "  342,\n",
              "  31531,\n",
              "  1919,\n",
              "  627,\n",
              "  20523,\n",
              "  291,\n",
              "  25,\n",
              "  2684,\n",
              "  527,\n",
              "  8071,\n",
              "  482,\n",
              "  1973,\n",
              "  34292,\n",
              "  11,\n",
              "  51677,\n",
              "  11,\n",
              "  264,\n",
              "  18537,\n",
              "  382,\n",
              "  34192,\n",
              "  279,\n",
              "  2768,\n",
              "  11914,\n",
              "  1701,\n",
              "  279,\n",
              "  5718,\n",
              "  7633,\n",
              "  3485,\n",
              "  1473,\n",
              "  39,\n",
              "  20782,\n",
              "  128009,\n",
              "  128006,\n",
              "  78191,\n",
              "  128007,\n",
              "  271,\n",
              "  90,\n",
              "  330,\n",
              "  10090,\n",
              "  4424,\n",
              "  794,\n",
              "  330,\n",
              "  39,\n",
              "  20782,\n",
              "  498,\n",
              "  330,\n",
              "  20523,\n",
              "  291,\n",
              "  4424,\n",
              "  794,\n",
              "  330,\n",
              "  9906,\n",
              "  1,\n",
              "  335],\n",
              " 'total_duration': 4896096125,\n",
              " 'load_duration': 3254583458,\n",
              " 'prompt_eval_count': 206,\n",
              " 'prompt_eval_duration': 1068287500,\n",
              " 'eval_count': 19,\n",
              " 'eval_duration': 571393042}"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llama3_engine.send_correct_request(\"Helo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 00:01:11,833 - INFO - Llama3InferenceEngine - correct_sentence - Correcting sentence: Helo\n",
            "2025-05-31 00:01:12,732 - INFO - Llama3InferenceEngine - correct_sentence - Corrected sentence: Hello\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Hello'"
            ]
          },
          "execution_count": 215,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llama3_engine.correct_sentence(\"Helo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do the evaluation, we need to choose some metrics to perform the task. For this, we will do it using three baselines:\n",
        "\n",
        "- Exact Match\n",
        "\n",
        "Why do we use this metrics to evaluate GEC?\n",
        "Exact match is pretty strict and shows how often the system produces a fully corrected sentence comparing with the test data. 100% deterministic so it is good for systems where only perfect corrections are allowed.\n",
        "- Gleu\n",
        "\n",
        "Why do we use it?\n",
        "It is a sentence level variant of BLEU, it is less strict than exact match and reflecs better incremental improvements, not a 100% deterministic correction.\n",
        "\n",
        "\n",
        "- ERRANT score\n",
        "Why do we use it?\n",
        "Because it is a standard metric for GEC, evaluates how well the system identifies and corrects specific errors. It provides a detailed edit level assement.\n",
        "Basically it compare edits between the system and a reference m2 files (we had to create) \n",
        "\n",
        "\n",
        "\n",
        "Note:\n",
        "*The first two evaluations are done with the evaluator class defined below, using the inference data included in the repository (fce_predicted.csv for t5, fce_predicted_llama.csv for llama3). The data was generated using a script included in the repository (/scripts/predictions_builder), instructions about how to use the script are there.*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.translate.gleu_score import sentence_gleu\n",
        "from typing import Optional\n",
        "import re\n",
        "\n",
        "class Evaluator:\n",
        "    \"\"\" Evaluator class for evaluating the performance of the inference engine on a dataset.\n",
        "    Args:\n",
        "        inference_engine (BaseInferenceEngine): The inference engine to use for evaluation.\n",
        "        n_samples (Optional[int]): Number of samples to evaluate. If None, evaluates the entire test set. Default is None.\n",
        "        predicted_dataset (Optional[DatasetDict]): A precomputed dataset with predictions. If provided, it will use this dataset instead of running inference. Default is None.\n",
        "        dataset (DatasetDict): The dataset to evaluate. It also runs inference.\n",
        "    \"\"\"\n",
        "    def __init__(self, inference_engine: BaseInferenceEngine, n_samples: Optional[int] = None, predicted_dataset: Optional[DatasetDict] = None, dataset: Optional[DatasetDict] = None):\n",
        "        # self.dataset = dataset\n",
        "        self.engine = inference_engine\n",
        "        self.n_samples = n_samples\n",
        "        self.test_data, self.references, self.predictions = None, None, None\n",
        "        if dataset is None and predicted_dataset is None:\n",
        "            raise ValueError(\"Either dataset or predicted_dataset must be provided.\")\n",
        "        if predicted_dataset is not None and isinstance(predicted_dataset, DatasetDict):\n",
        "            self.test_data = predicted_dataset['test']['source']\n",
        "            self.references = predicted_dataset['test']['target']\n",
        "            self.predictions = predicted_dataset['test']['prediction']\n",
        "        elif dataset is not None and isinstance(dataset, DatasetDict):\n",
        "            self.dataset = dataset\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_text(text):\n",
        "        # Remove spaces before punctuation and ensure single space after\n",
        "        text = re.sub(r'\\s+([.,!?;:\"])', r'\\1', text)\n",
        "        text = re.sub(r'([.,!?;:\"])([^\\s])', r'\\1 \\2', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _get_samples(self) -> Tuple[List[str], List[str], List[str]]:\n",
        "        \"\"\" Fetch samples from the test set.\"\"\"\n",
        "        test_data = self.dataset['test'] if self.test_data is None else self.dataset['test']\n",
        "        n = min(self.n_samples, len(test_data)) if (self.n_samples is not None and self.n_samples > 0) else len(test_data)\n",
        "        self.logger.info(f\"Fetching {n} samples from the test set.\")\n",
        "        sample_sentences = [test_data[i]['source'] for i in range(n)]\n",
        "        references = [test_data[i]['target'] for i in range(n)]\n",
        "        predictions = self.engine.batch_correct(sample_sentences)\n",
        "        return sample_sentences, references, predictions\n",
        "    \n",
        "    def  _get_samples_if_not_available(self):\n",
        "        \"\"\"\n",
        "        Check if samples are available.\n",
        "        \"\"\"\n",
        "        if self.test_data is None or self.references is None or self.predictions is None:\n",
        "            self.logger.warning(\"Samples not available. Fetching samples...\")\n",
        "            self.test_data, self.references, self.predictions = self._get_samples()\n",
        "        else:\n",
        "            self.logger.info(\"Samples already available. Using cached samples.\")\n",
        "\n",
        "    def evaluate_accuracy(self) -> float:\n",
        "        \"\"\" \n",
        "        Evaluate the accuracy of the predictions against the references.\n",
        "        It computes the exact match accuracy.\n",
        "        Returns:\n",
        "            float: The exact match accuracy.\n",
        "        \"\"\"\n",
        "        self._get_samples_if_not_available()\n",
        "        norm_refs = [self.normalize_text(str(ref)) for ref in self.references]\n",
        "        norm_preds = [self.normalize_text(str(pred)) for pred in self.predictions]\n",
        "        accuracy = accuracy_score(norm_refs, norm_preds)\n",
        "        self.logger.info(f\"Exact match accuracy on test set: {accuracy:.4f}\")\n",
        "        return accuracy\n",
        "\n",
        "    def evaluate_gleu(self):\n",
        "        \"\"\"\n",
        "        Evaluate the GLEU score of the predictions against the references.\n",
        "        It uses the nltk library to compute the GLEU score.\n",
        "        Returns:\n",
        "            float: The average GLEU score across all samples.\n",
        "            \n",
        "        \"\"\"\n",
        "        self._get_samples_if_not_available()\n",
        "        gleu_scores = []\n",
        "        for pred, ref in zip(self.predictions, self.references):\n",
        "            ref_tokens = self.normalize_text(str(ref)).split()\n",
        "            pred_tokens = self.normalize_text(str(pred)).split()\n",
        "            gleu = sentence_gleu([ref_tokens], pred_tokens)\n",
        "            gleu_scores.append(gleu)\n",
        "        avg_gleu = sum(gleu_scores) / len(gleu_scores) if gleu_scores else 0.0\n",
        "        self.logger.info(f\"Average GLEU score on test set: {avg_gleu:.4f}\")\n",
        "        return avg_gleu\n",
        "\n",
        "    # --- ASYNC BATCH INFERENCE FOR LLAMA3 ---\n",
        "    async def evaluate_accuracy_async(self)-> Optional[float]:\n",
        "        \"\"\" Evaluate the accuracy of the predictions against the references using async batch inference.\n",
        "        It computes the exact match accuracy.\n",
        "        This method is specifically designed for engines that support async batch inference.\n",
        "\n",
        "        Returns:\n",
        "            Optional[float]: The exact match accuracy if async batch inference is available, otherwise None.\n",
        "        \"\"\"\n",
        "        # Fetch samples (sentences and references)\n",
        "        test_data = self.dataset['test']\n",
        "        n = min(self.n_samples, len(test_data)) if (self.n_samples is not None and self.n_samples > 0) else len(test_data)\n",
        "        self.logger.info(f\"Fetching {n} samples from the test set (async).\")\n",
        "        sample_sentences = [test_data[i]['source'] for i in range(n)]\n",
        "        references = [test_data[i]['target'] for i in range(n)]\n",
        "        # Use async batch correct if available\n",
        "        if hasattr(self.engine, \"async_batch_correct\"):\n",
        "            predictions = await self.engine.async_batch_correct(sample_sentences)\n",
        "            self.test_data, self.references, self.predictions = sample_sentences, references, predictions\n",
        "            norm_refs = [self.normalize_text(ref) for ref in self.references]\n",
        "            norm_preds = [self.normalize_text(pred) for pred in self.predictions]\n",
        "            accuracy = accuracy_score(norm_refs, norm_preds)\n",
        "            self.logger.info(f\"Exact match accuracy on test set: {accuracy:.4f}\")\n",
        "            return accuracy\n",
        "        else:\n",
        "            self.logger.warning(\"Async batch inference not available for this engine.\")\n",
        "            return None\n",
        "\n",
        "    async def evaluate_gleu_async(self)-> Optional[float]:\n",
        "        \"\"\" Evaluate the GLEU score of the predictions against the references using async batch inference.\n",
        "        It uses the nltk library to compute the GLEU score.\n",
        "        This method is specifically designed for engines that support async batch inference.\n",
        "\n",
        "        Returns:\n",
        "            Optional[float]: The average GLEU score across all samples if async batch inference is available, otherwise None.\n",
        "        \"\"\"\n",
        "        # Fetch samples (sentences and references)\n",
        "        test_data = self.dataset['test']\n",
        "        n = min(self.n_samples, len(test_data)) if (self.n_samples is not None and self.n_samples > 0) else len(test_data)\n",
        "        self.logger.info(f\"Fetching {n} samples from the test set (async).\")\n",
        "        sample_sentences = [test_data[i]['source'] for i in range(n)]\n",
        "        references = [test_data[i]['target'] for i in range(n)]\n",
        "        # Use async batch correct if available\n",
        "        if hasattr(self.engine, \"async_batch_correct\"):\n",
        "            predictions = await self.engine.async_batch_correct(sample_sentences)\n",
        "            self.test_data, self.references, self.predictions = sample_sentences, references, predictions\n",
        "            gleu_scores = []\n",
        "            for pred, ref in zip(self.predictions, self.references):\n",
        "                ref_tokens = self.normalize_text(ref).split()\n",
        "                pred_tokens = self.normalize_text(pred).split()\n",
        "                gleu = sentence_gleu([ref_tokens], pred_tokens)\n",
        "                gleu_scores.append(gleu)\n",
        "            avg_gleu = sum(gleu_scores) / len(gleu_scores) if gleu_scores else 0.0\n",
        "            self.logger.info(f\"Average GLEU score on test set: {avg_gleu:.4f}\")\n",
        "            return avg_gleu\n",
        "        else:\n",
        "            self.logger.warning(\"Async batch inference not available for this engine.\")\n",
        "            return None\n",
        "    \n",
        "    @staticmethod\n",
        "    def evaluate_single(pred, ref):\n",
        "        norm_pred = Evaluator.normalize_text(pred)\n",
        "        norm_ref = Evaluator.normalize_text(ref)\n",
        "        exact = int(norm_pred == norm_ref)\n",
        "        gleu = sentence_gleu([norm_ref.split()], norm_pred.split())\n",
        "        return {\"exact_match\": exact, \"gleu\": gleu}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating FCE test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the test dataset (FCE with T5) using GLEU and exact match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "## This dataset was generated using the following command:\n",
        "# python3 -m scripts.predictions_builder medical t5 fce /Users/isaac/Developer/GEC-system/data/fce_predicted.csv\n",
        "\n",
        "import pandas as pd\n",
        "predicted_fce_df = pd.read_csv(os.path.join(\"/Users/isaac/Developer/GEC-system/data/fce_predicted.csv\"))\n",
        "predicted_fce_dataset = DatasetDict({\n",
        "    'test': Dataset.from_pandas(predicted_fce_df)\n",
        "})\n",
        "\n",
        "\n",
        "# If you don't want to use it, comment the previous lines and uncomment the following \n",
        "# preprocessed_dataset = load_from_disk(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"preprocessed_fce_dataset\"))\n",
        "# from datasets import load_from_disk\n",
        "# preprocessed_dataset = load_from_disk(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"preprocessed_fce_dataset\"))\n",
        "# t5_inference_engine = T5InferenceEngine(model_dir=model_dir, max_length=650)\n",
        "\n",
        "# t5_evaluator= Evaluator(\n",
        "#     inference_engine=t5_inference_engine,\n",
        "#     dataset=preprocessed_dataset\n",
        "# )\n",
        "# accuracy = t5_evaluator.evaluate_accuracy()\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "# gleu = t5_evaluator.evaluate_gleu()\n",
        "# print(\"GLEU:\", gleu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "t5_fce_evaluator = Evaluator(t5_inference_engine, predicted_dataset=predicted_fce_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 19:29:45,350 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-06-01 19:29:45,416 - INFO - Evaluator - evaluate_accuracy - Exact match accuracy on test set: 0.3833\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.38330241187384045"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t5_fce_evaluator.evaluate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 19:29:46,749 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-06-01 19:29:46,898 - INFO - Evaluator - evaluate_gleu - Average GLEU score on test set: 0.7843\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7842619115222491"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t5_fce_evaluator.evaluate_gleu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This are kinda good results when using a simple model. Strict matches are good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source: I will give you now some useful information.\n",
            "Target: I will now give you some useful information.\n",
            "Prediction: I will give you now some useful information.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.5000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: But also, the museum have a lot of gardens with several statues.\n",
            "Target: But also the museum has a lot of gardens with several statues.\n",
            "Prediction: But also, the museum has a lot of gardens with several statues.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.8333\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: The hotel is just three busstops away from our college.\n",
            "Target: The hotel is just three bus stops away from our college.\n",
            "Prediction: The hotel is just three bus stops away from our college.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: I hope to give you, all the information need and please, if you want more information or somenthing is not clear, please do n't esitate to contact me again.\n",
            "Target: I hope to give you all the information you need and, please, if you want more information or something is not clear, please do n't hesitate to contact me again.\n",
            "Prediction: I hope to give you all the information needed and please, if you want more information or something is not clear, please don't hesitate to contact me again.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.7193\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: This is an informal party.\n",
            "Target: This is an informal party.\n",
            "Prediction: This is an informal party.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: The second one is the lamp, the electricity that is very important in our life.\n",
            "Target: The second one is the lamp, the electricity that is very important in our life.\n",
            "Prediction: The second one is the lamp, the electricity that is very important in our life.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: 12th June 2001\n",
            "Target: 12th June 2001\n",
            "Prediction: 12th June 2001\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: I am writing to give you some advice of an international student conference.\n",
            "Target: I am writing to give you some advice about the international student conference.\n",
            "Prediction: I am writing to give you some advice about an international student conference.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.7826\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: Firstly, the Palace hotel is your accommodation which you have booked.\n",
            "Target: Firstly, the Palace hotel is your accommodation which you have booked.\n",
            "Prediction: Firstly, the Palace hotel is your accommodation which you have booked.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: Dear Mrs Smith,\n",
            "Target: Dear Mrs Smith,\n",
            "Prediction: Dear Mrs Smith,\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "sample_indices = random.sample(range(len(predicted_fce_dataset['test'])), 10)\n",
        "for idx in sample_indices:\n",
        "    src = Evaluator.normalize_text(predicted_fce_dataset['test'][idx]['source'])\n",
        "    tgt = Evaluator.normalize_text(predicted_fce_dataset['test'][idx]['target'])\n",
        "    pred = Evaluator.normalize_text(predicted_fce_dataset['test'][idx]['prediction'])\n",
        "    print(f\"Source: {src}\\nTarget: {tgt}\\nPrediction: {pred}\\n\")\n",
        "    result = t5_fce_evaluator.evaluate_single(pred, tgt)\n",
        "    print(f\"Exact Match: {result['exact_match']}, GLEU: {result['gleu']:.4f}\\n{'-'*60}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating test data (FCE with Llama 3) using GLEU and exact match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 14:35:16,007 - INFO - Llama3InferenceEngine - __init__ - Llama3InferenceEngine initialized with model endpoint: http://127.0.0.1:11434/api/generate and prompt path: /Users/isaac/Developer/GEC-system/config/prompt_general.txt, options: {'temperature': 0.0, 'seed': 123, 'top_k': 10, 'top_p': 0.5}\n",
            "2025-06-01 14:35:16,021 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-06-01 14:35:16,065 - INFO - Evaluator - evaluate_accuracy - Exact match accuracy on test set: 0.1325\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.13246753246753246"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_fce_llama_df = pd.read_csv(\"/Users/isaac/Developer/GEC-system/data/fce_predicted_llama.csv\")\n",
        "llama3_engine = Llama3InferenceEngine(model_endpoint=LLAMA3_ENDPOINT, prompt_path=GENERAL_PROMPT_PATH)\n",
        "\n",
        "llama3_fce_evaluator = Evaluator(llama3_engine, predicted_dataset=DatasetDict({'test': Dataset.from_pandas(predicted_fce_llama_df)}))\n",
        "llama3_fce_evaluator.evaluate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 14:35:18,217 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-06-01 14:35:18,367 - INFO - Evaluator - evaluate_gleu - Average GLEU score on test set: 0.6072\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.6072433371173874"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llama3_fce_evaluator.evaluate_gleu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Worse results in exact match due to a prompt bad structured with a lot of edge cases..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating our medical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we want to evaluate with our medical data to see how it behaves..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['source', 'target'],\n",
              "    num_rows: 204\n",
              "})"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Read your medical data CSV (update the path as needed)\n",
        "medical_df = pd.read_csv(\"/Users/isaac/Developer/GEC-system/data/data.csv\")  # columns should be ['source', 'target']\n",
        "medical_df.rename(columns={'incorrect_sentence': 'source', 'correct_sentence': 'target'}, inplace=True)\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "medical_data = Dataset.from_pandas(medical_df)\n",
        "medical_dataset_dict = DatasetDict({'test': medical_data})\n",
        "\n",
        "\n",
        "medical_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating with T5 using exact match and GLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Precomputed predictions for the medical dataset\n",
        "\n",
        "medical_t5_predictions_df = pd.read_csv(\"/Users/isaac/Developer/GEC-system/data/medical_predicted.csv\")\n",
        "\n",
        "t5_medical_evaluator = Evaluator(t5_inference_engine, predicted_dataset=DatasetDict({'test': Dataset.from_pandas(medical_t5_predictions_df)}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1323529411764706"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t5_medical_evaluator.evaluate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7898051106345906"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t5_medical_evaluator.evaluate_gleu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bad exact match due to lack of medical context..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating with Llama3 using exact match and GLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 13:08:28,092 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-06-01 13:08:28,099 - INFO - Evaluator - evaluate_accuracy - Exact match accuracy on test set: 0.5049\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5049019607843137"
            ]
          },
          "execution_count": 374,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Precomputed predictions for the medical dataset using Llama3\n",
        "\n",
        "llama3_medical_predictions_df = pd.read_csv(\"/Users/isaac/Developer/GEC-system/data/medical_predicted_llama.csv\")\n",
        "\n",
        "llama3_medical_evaluator = Evaluator(llama3_engine, predicted_dataset=DatasetDict({'test': Dataset.from_pandas(llama3_medical_predictions_df)}))\n",
        "llama3_medical_evaluator.evaluate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 13:08:31,968 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-06-01 13:08:31,988 - INFO - Evaluator - evaluate_gleu - Average GLEU score on test set: 0.8691\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.869140455496451"
            ]
          },
          "execution_count": 375,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llama3_medical_evaluator.evaluate_gleu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pretty good results due to prompt tunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using ERRANT Scorer on test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate gold FCE data using our framework \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to generate our own gold data using the other framework due to some postprocessing we're doing (like standard punctuation removing spaces between commas or dots), and also i noticed some examples where the gold data annotation of bea19 is 100% not compatible with the m2 files generated but at the end of the day the edits are the same but annotated different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed_dataset = load_from_disk(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce/preprocessed_fce_dataset\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate gold FCE data using our framework \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(os.path.join(os.getcwd(), \"data\", \"fce_predicted.csv\"))[\"target\"].tolist()\n",
        "\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce_gold.txt\"), 'w') as f:\n",
        "    for sentence in data:\n",
        "        f.write(Evaluator.normalize_text(sentence) + \"\\n\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ]
        }
      ],
      "source": [
        "!errant_parallel -orig ./data/fce_wrong.txt -cor ./data/fce_gold.txt -out ./data/m2/fce_gold.m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data is already predicted and stored as csv in data/fce_predicted.csv. Now we're gonna convert the predictions as txt as expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read your medical data CSV (update the path as needed)\n",
        "fce_predicted_df_t5 = pd.read_csv(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce_predicted.csv\"))  # columns should be ['source', 'target']\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce_predicted_t5.txt\"), 'w') as f:\n",
        "    for sentence in fce_predicted_df_t5['prediction']:\n",
        "        f.write(Evaluator.normalize_text(sentence) + '\\n')\n",
        "\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce_wrong.txt\"), 'w') as f:\n",
        "    for sentence in fce_predicted_df_t5['source']:\n",
        "        f.write(Evaluator.normalize_text(sentence) + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate system M2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install --upgrade --force-reinstall numpy h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ]
        }
      ],
      "source": [
        "!errant_parallel -orig ./data/fce_wrong.txt -cor ./data/fce_predicted_t5.txt -out ./data/m2/fce_predicted_t5.m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run it as script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "1452\t1411\t3312\t0.5072\t0.3048\t0.4477\n",
            "==============================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!errant_compare -hyp ./data/m2/fce_predicted_t5.m2 -ref ./data/m2/fce_gold.m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, we see is a solid results for a grammatical error correction (GEC) system, especially if you are using a small or moderately sized model, or if this is your first or baseline system.\n",
        "\n",
        "Precision (0.5122) is much higher than recall (0.2827), meaning your system is conservative: it makes fewer edits, but they are more likely to be correct.\n",
        "\n",
        "Recall could be improved (your system is missing some errors), but this is common for many GEC systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLAMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running analysis over the most common edits in the fce data so we can finetune the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('noop', 903),\n",
              " ('R:OTHER', 617),\n",
              " ('R:NOUN', 576),\n",
              " ('R:SPELL', 423),\n",
              " ('M:DET', 335),\n",
              " ('R:ORTH', 263),\n",
              " ('R:PREP', 254),\n",
              " ('R:VERB', 233),\n",
              " ('R:VERB:TENSE', 165),\n",
              " ('R:DET', 134)]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M2DatasetLoader.most_common_edit_types(gold_m2_path=\"./data/m2/fce_gold.m2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "### LLAMA\n",
        "import pandas as pd\n",
        "\n",
        "# Read your medical data CSV (update the path as needed)\n",
        "fce_predicted_df_llama = pd.read_csv(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce_predicted_llama.csv\"))  # columns should be ['source', 'target']\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce_predicted_llama.txt\"), 'w') as f:\n",
        "    for sentence in fce_predicted_df_llama['prediction']:\n",
        "        f.write(Evaluator.normalize_text(str(sentence)) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ]
        }
      ],
      "source": [
        "!errant_parallel -orig ./data/fce_wrong.txt -cor ./data/fce_predicted_llama.txt -out ./data/m2/fce_predicted_llama.m2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "1667\t3509\t3097\t0.3221\t0.3499\t0.3273\n",
            "==============================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!errant_compare -hyp ./data/m2/fce_predicted_llama.m2 -ref ./data/m2/fce_gold.m2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source: No jeans and tee - shirts.\n",
            "Target: No jeans or tee shirts.\n",
            "Prediction: No jeans and T-shirts.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.2143\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: It is going to be very lovely and enjoyable because I have lots of surprises for students.\n",
            "Target: It is going to be very lovely and enjoyable because I have lots of surprises for the students.\n",
            "Prediction: It is going to be very lovely and enjoyable because I have lots of surprises for the students.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: With reference to the information that you had requested, the hotel that had been booked is the Holiday Inn, in New Port and to get to the University of Wales wich is not far from the hotel, you only need to take a divertion where clearly indicate Carleon and once that you are on the main road all you need to do is to follow the country road which take you direct to the place.\n",
            "Target: With reference to the information that you have requested, the hotel that has been booked is the Holiday Inn, in New Port and to get to the University of Wales, which is not far from the hotel, you only need to take a diversion where clearly indicate Carleon and once you are on the main road, all you need to do is to follow the country road which takes you directly to the place.\n",
            "Prediction: With reference to the information that you had requested, the hotel that had been booked is the Holiday Inn, in Newport, and to get to the University of Wales, which is not far from the hotel, you only need to take a diversion where clearly indicated Carleon, and once you are on the main road, all you need to do is follow the country road, which takes you directly to the place.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.7655\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: There are also restaurent 's and modern gallaries.\n",
            "Target: there are also restaurants and modern galleries.\n",
            "Prediction: There are also restaurants and modern galleries.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.8182\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: I hope you will be happy with our conference and party and etc.\n",
            "Target: I hope you will be happy with our conference and party etc.\n",
            "Prediction: I hope you will be happy with our conference and party, etc.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.8333\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: Firstly, the Palace Hotel is waiting for you and your students.\n",
            "Target: Firstly, the Palace Hotel is waiting for you and your students.\n",
            "Prediction: Firstly, the Palace Hotel is waiting for you and your students.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: There are exhibited historcall things, picthurs and clothes.\n",
            "Target: Historical things, pictures and clothes are exhibited there.\n",
            "Prediction: There are exhibited historical things, pictures and clothes.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.3462\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: If there is someone who does n't agree with me, he or she should try to live one month without using the oven.\n",
            "Target: If there is someone who does n't agree with me, he or she should try to live for one month without using an oven.\n",
            "Prediction: If there is someone who doesn't agree with me, he or she should try to live one month without using the oven.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.6556\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: The building is huge with high technology and construction and a tight security.\n",
            "Target: The building is huge with modern technology and construction and tight security.\n",
            "Prediction: The building is huge with high-tech construction and tight security\n",
            "\n",
            "Exact Match: 0, GLEU: 0.4762\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: Every thing that you need you can find it in internet.\n",
            "Target: You can find everything that you need on the Internet.\n",
            "Prediction: Every thing that you need you can find it on the internet.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.2857\n",
            "------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "sample_indices = random.sample(range(len(fce_predicted_df_llama)), 10)\n",
        "for idx in sample_indices:\n",
        "    src = Evaluator.normalize_text(fce_predicted_df_llama['source'][idx])\n",
        "    tgt = Evaluator.normalize_text(fce_predicted_df_llama['target'][idx])\n",
        "    pred = Evaluator.normalize_text(fce_predicted_df_llama['prediction'][idx])\n",
        "    print(f\"Source: {src}\\nTarget: {tgt}\\nPrediction: {pred}\\n\")\n",
        "    result = t5_fce_evaluator.evaluate_single(pred, tgt)\n",
        "    print(f\"Exact Match: {result['exact_match']}, GLEU: {result['gleu']:.4f}\\n{'-'*60}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is making some mistakes due to some lack of context and training. Still baseline (not that bad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Errant Score on medical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate m2 files including gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "medical_data = pd.read_csv(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_predicted.csv\"))\n",
        "\n",
        "\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_wrong.txt\"), 'w') as f:\n",
        "    for sentence in medical_data['source']:\n",
        "        f.write(Evaluator.normalize_text(sentence) + '\\n')\n",
        "        \n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_gold.txt\"), 'w') as f:\n",
        "    for sentence in medical_data['target']:\n",
        "        f.write(sentence + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ]
        }
      ],
      "source": [
        "!errant_parallel -orig ./data/medical_wrong.txt -cor ./data/medical_gold.txt -out ./data/m2/medical_gold.m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read your medical data CSV (update the path as needed)\n",
        "medical_predicted_df_t5 = pd.read_csv(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_predicted.csv\"))  # columns should be ['source', 'target']\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_predicted_t5.txt\"), 'w') as f:\n",
        "    for sentence in medical_predicted_df_t5['prediction']:\n",
        "        f.write(Evaluator.normalize_text(sentence) + '\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ]
        }
      ],
      "source": [
        "!errant_parallel -orig ./data/medical_wrong.txt -cor ./data/medical_predicted_t5.txt -out ./data/m2/medical_predicted_t5.m2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "31\t184\t208\t0.1442\t0.1297\t0.141\n",
            "==============================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!errant_compare -hyp ./data/m2/medical_predicted_t5.m2 -ref ./data/m2/medical_gold.m2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source: Patient develop diabetic ketoacidosis with severe dehydration and electrolyte imbalances requiring intensive care management.\n",
            "Target: The patient developed diabetic ketoacidosis with severe dehydration and electrolyte imbalances requiring intensive care management.\n",
            "Prediction: Patient develops diabetic ketoacidosis with severe dehydration and electrolyte imbalances requiring intensive care management.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.7778\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: The radiation oncologist recommend intensity-modulated radiation therapy for locally advanced prostate adenocarcinoma.\n",
            "Target: The radiation oncologist recommended intensity-modulated radiation therapy for locally advanced prostate adenocarcinoma.\n",
            "Prediction: The radiation oncologist recommend intensity-modulated radiation therapy for locally advanced prostate adenocarcinoma.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.7619\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: Slit-lamp examination reveal anterior uveitis with keratic precipitates and posterior synechiae formation.\n",
            "Target: Slit-lamp examination revealed anterior uveitis with keratic precipitates and posterior synechiae formation.\n",
            "Prediction: Slit-lamp examination revealed anterior uveitis with keratic precipitates and posterior synechiae formation.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: She were admitted to the coronary care unit yesterday following acute ST-elevation myocardial infarction.\n",
            "Target: She was admitted to the coronary care unit yesterday following acute ST-elevation myocardial infarction.\n",
            "Prediction: She was admitted to the coronary care unit yesterday following acute ST-elevation myocardial infarction.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: The gastroenterologist performed endoscopic retrograde cholangiopancreatography to evaluate for choledocholithiasis and pancreatic ductal adenocarcinoma.\n",
            "Target: The gastroenterologist performed endoscopic retrograde cholangiopancreatography to evaluate for choledocholithiasis and pancreatic ductal adenocarcinoma.\n",
            "Prediction: The gastroenterologist performed endoscopic retrograde cholangiopancreatography to evaluate for choledocholithiasis and pancreatic ductal adenocarcinoma.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: Positron emission tomography scan reveal hypermetabolic activity in mediastinal lymph nodes suggesting possible malignancy.\n",
            "Target: Positron emission tomography scan revealed hypermetabolic activity in mediastinal lymph nodes suggesting possible malignancy.\n",
            "Prediction: Positron emission tomography scans reveal hypermetabolic activity in mediastinal lymph nodes suggesting possible malignancy.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.7200\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: Endoscopic ultrasound demonstrate pancreatic mass with regional lymphadenopathy requiring endoscopic ultrasound-guided fine needle aspiration.\n",
            "Target: Endoscopic ultrasound demonstrated pancreatic mass with regional lymphadenopathy requiring endoscopic ultrasound-guided fine needle aspiration.\n",
            "Prediction: Endoscopic ultrasound shows pancreatic mass with regional lymphadenopathy requiring endoscopic ultrasound-guided fine needle aspiration.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.8200\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: The forensic pathologist identify blunt force trauma to the head with subdural hemorrhage and cerebral contusions.\n",
            "Target: The forensic pathologist identified blunt force trauma to the head with subdural hemorrhage and cerebral contusions.\n",
            "Prediction: The forensic pathologist identified blunt force trauma to the head with subdural hemorrhage and cerebral contusions.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: The radiologist report multiple metastatic lesions in the liver parenchyma consistent with colorectal adenocarcinoma spread.\n",
            "Target: The radiologist reported multiple metastatic lesions in the liver parenchyma consistent with colorectal adenocarcinoma spread.\n",
            "Prediction: The radiologist report multiple metastatic lesions in the liver parenchyma consistent with colorectal adenocarcinoma spread.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.8333\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: He is taking metformin hydrochloride and lisinopril since the diagnosis of diabetes mellitus type 2 was established two weeks ago.\n",
            "Target: He has been taking metformin hydrochloride and lisinopril for two weeks since the diagnosis of diabetes mellitus type 2 was established.\n",
            "Prediction: He is taking metformin hydrochloride and lisinopril since the diagnosis of diabetes mellitus type 2 was established two weeks ago.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.6154\n",
            "------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "sample_indices = random.sample(range(len(medical_predicted_df_t5)), 10)\n",
        "for idx in sample_indices:\n",
        "    src = Evaluator.normalize_text(medical_predicted_df_t5['source'][idx])\n",
        "    tgt = Evaluator.normalize_text(medical_predicted_df_t5['target'][idx])\n",
        "    pred = Evaluator.normalize_text(medical_predicted_df_t5['prediction'][idx])\n",
        "    print(f\"Source: {src}\\nTarget: {tgt}\\nPrediction: {pred}\\n\")\n",
        "    result = Evaluator.evaluate_single(pred, tgt)\n",
        "    print(f\"Exact Match: {result['exact_match']}, GLEU: {result['gleu']:.4f}\\n{'-'*60}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lack of medical data while training is giving a bad score "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLAMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running analysis over the most common edits in the medical data so we can finetune the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('R:VERB:TENSE', 177),\n",
              " ('M:DET', 36),\n",
              " ('R:VERB:FORM', 7),\n",
              " ('R:VERB:SVA', 6),\n",
              " ('R:PREP', 5),\n",
              " ('M:OTHER', 1),\n",
              " ('R:OTHER', 1),\n",
              " ('U:OTHER', 1),\n",
              " ('M:VERB:TENSE', 1),\n",
              " ('R:ADV', 1)]"
            ]
          },
          "execution_count": 358,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M2DatasetLoader.most_common_edit_types(gold_m2_path=\"./data/m2/medical_gold.m2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read your medical data CSV (update the path as needed)\n",
        "medical_predicted_df_llama = pd.read_csv(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_predicted_llama.csv\"))  # columns should be ['source', 'target']\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_predicted_llama.txt\"), 'w') as f:\n",
        "    for sentence in medical_predicted_df_llama['prediction']:\n",
        "        f.write(Evaluator.normalize_text(str(sentence)) + '\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ]
        }
      ],
      "source": [
        "!errant_parallel -orig ./data/medical_wrong.txt -cor ./data/medical_predicted_llama.txt -out ./data/m2/medical_predicted_llama.m2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('R:VERB:TENSE', 155),\n",
              " ('M:DET', 60),\n",
              " ('R:NOUN:NUM', 38),\n",
              " ('R:VERB:SVA', 21),\n",
              " ('R:NOUN', 17),\n",
              " ('R:VERB:FORM', 10),\n",
              " ('R:SPELL', 8),\n",
              " ('R:OTHER', 8),\n",
              " ('R:PREP', 5),\n",
              " ('M:OTHER', 4)]"
            ]
          },
          "execution_count": 389,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M2DatasetLoader.most_common_edit_types(gold_m2_path=\"./data/m2/medical_predicted_llama.m2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "181\t156\t58\t0.5371\t0.7573\t0.5703\n",
            "==============================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!errant_compare -hyp ./data/m2/medical_predicted_llama.m2 -ref ./data/m2/medical_gold.m2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source: Electrophysiology study show prolonged QT interval and increased risk for torsades de pointes arrhythmia.\n",
            "Target: Electrophysiology study showed prolonged QT interval and increased risk for torsades de pointes arrhythmia.\n",
            "Prediction: Electrophysiology studies showed prolonged QT interval and an increased risk for torsades de pointes arrhythmia.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.6852\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: Spirometry testing show severe obstructive pattern with reduced forced expiratory volume in one second.\n",
            "Target: Spirometry testing showed severe obstructive pattern with reduced forced expiratory volume in one second.\n",
            "Prediction: Spirometry testing showed severe obstructive pattern with reduced forced expiratory volume in one second.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: Systolic and diastolic blood pressure is in normal range following administration of antihypertensive medication regimen.\n",
            "Target: The systolic and diastolic blood pressure is in the normal range following administration of antihypertensive medication regimen.\n",
            "Prediction: Systolic and diastolic blood pressure was in a normal range following the administration of an antihypertensive medication regimen.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.4091\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: The nuclear medicine physician perform parathyroid scintigraphy for localization of adenomatous tissue prior to surgical resection.\n",
            "Target: The nuclear medicine physician performed parathyroid scintigraphy for localization of adenomatous tissue prior to surgical resection.\n",
            "Prediction: The nuclear medicine physician performed parathyroid scintigraphy for the localization of adenomatous tissue prior to surgical resection.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.8387\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: The rheumatologist diagnose systemic lupus erythematosus based on positive antinuclear antibody and anti-double-stranded DNA levels.\n",
            "Target: The rheumatologist diagnosed systemic lupus erythematosus based on positive antinuclear antibody and anti-double-stranded DNA levels.\n",
            "Prediction: The rheumatologist diagnosed systemic lupus erythematosus based on positive antinuclear antibody and anti-double-stranded DNA levels.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: The neonatologist diagnose respiratory distress syndrome in preterm infant requiring surfactant replacement therapy.\n",
            "Target: The neonatologist diagnosed respiratory distress syndrome in preterm infant requiring surfactant replacement therapy.\n",
            "Prediction: The neonatologists diagnosed respiratory distress syndrome in a preterm infant requiring surfactant replacement therapy.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.6600\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: Fluorodeoxyglucose positron emission tomography show increased metabolic activity consistent with viable myocardium.\n",
            "Target: Fluorodeoxyglucose positron emission tomography showed increased metabolic activity consistent with viable myocardium.\n",
            "Prediction: Fluorodeoxyglucose positron emission tomography showed increased metabolic activity consistent with viable myocardium.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: The occupational therapist recommend adaptive equipment and environmental modifications for activities of daily living.\n",
            "Target: The occupational therapist recommended adaptive equipment and environmental modifications for activities of daily living.\n",
            "Prediction: The occupational therapist recommends adaptive equipment and environmental modifications for activities of daily living.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.8000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: Magnetic resonance spectroscopy demonstrate reduced N-acetylaspartate levels consistent with neuronal loss and gliosis.\n",
            "Target: Magnetic resonance spectroscopy demonstrated reduced N-acetylaspartate levels consistent with neuronal loss and gliosis.\n",
            "Prediction: Magnetic resonance spectroscopy demonstrated reduced N-acetylaspartate levels consistent with neuronal loss and gliosis.\n",
            "\n",
            "Exact Match: 1, GLEU: 1.0000\n",
            "------------------------------------------------------------\n",
            "\n",
            "Source: Immunohistochemistry staining show positive cytokeratin and negative vimentin consistent with epithelial tumor origin.\n",
            "Target: Immunohistochemistry staining showed positive cytokeratin and negative vimentin consistent with epithelial tumor origin.\n",
            "Prediction: Immunohistochemistry staining showed positive cytokeratin and negative vimentin, consistent with epithelial tumor origin.\n",
            "\n",
            "Exact Match: 0, GLEU: 0.7826\n",
            "------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "sample_indices = random.sample(range(len(medical_predicted_df_llama)), 10)\n",
        "for idx in sample_indices:\n",
        "    src = Evaluator.normalize_text(medical_predicted_df_llama['source'][idx])\n",
        "    tgt = Evaluator.normalize_text(medical_predicted_df_llama['target'][idx])\n",
        "    pred = Evaluator.normalize_text(medical_predicted_df_llama['prediction'][idx])\n",
        "    print(f\"Source: {src}\\nTarget: {tgt}\\nPrediction: {pred}\\n\")\n",
        "    result = t5_fce_evaluator.evaluate_single(pred, tgt)\n",
        "    print(f\"Exact Match: {result['exact_match']}, GLEU: {result['gleu']:.4f}\\n{'-'*60}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kinda good results for only prompt engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How can we improve?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## General data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLAMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using a larger model, improving the prompt passing more examples, or even fine tune it with the bea19 data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We used T5 small, so we could try choosing T5 base or some other larger model, fine tune it again, leaving it more time (when i trained i only chose 3 epochs due to gpu limitations) of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Medical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For improving this model i suggest finetuning the model that has been already trained for correction in general data, using medical to see if we improve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming medical_df is already loaded and columns are ['source', 'target']\n",
        "import pandas as pd\n",
        "\n",
        "medical_df = pd.read_csv(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"data.csv\"))\n",
        "medical_df.rename(columns={'incorrect_sentence': 'source', 'correct_sentence': 'target'}, inplace=True)\n",
        "\n",
        "# Split into train (80%), validation (10%), test (10%)\n",
        "train_df, temp_df = train_test_split(medical_df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "medical_split_dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
        "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
        "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load the already trained T5 model and tokenizer, then retrain (finetune) with medical data\n",
        "\n",
        "# Load the previously trained model (from model_dir)\n",
        "\n",
        "finetune_model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "finetune_tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
        "\n",
        "# Create a new T5Preprocessor using the loaded tokenizer\n",
        "medical_preprocessor = T5Preprocessor(tokenizer=finetune_tokenizer)\n",
        "\n",
        "# Get max input length for medical data\n",
        "medical_max_length = T5Preprocessor._get_max_input_length(medical_split_dataset)\n",
        "\n",
        "# Preprocess the medical dataset\n",
        "medical_tokenized = medical_preprocessor.preprocess(medical_split_dataset, max_length=medical_max_length)\n",
        "\n",
        "# Define a new output directory for the finetuned model\n",
        "medical_finetune_output_dir = \"./models/t5_finetuned_medical\"\n",
        "\n",
        "# Create a new T5Trainer using the loaded model and tokenizer\n",
        "class CustomT5Trainer(T5Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.model = finetune_model\n",
        "        self.tokenizer = finetune_tokenizer\n",
        "\n",
        "medical_trainer = CustomT5Trainer(\n",
        "    output_dir=medical_finetune_output_dir,\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "# Train (finetune) the model on the medical data\n",
        "medical_model_dir, medical_model, medical_tokenizer = medical_trainer.train(medical_tokenized, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 22:01:59,929 - INFO - T5InferenceEngine - __init__ - T5 model loaded from ./t5_finetuned_medical/20250601-220037 with max length 650\n",
            "2025-06-01 22:01:59,934 - WARNING - Evaluator - _get_samples_if_not_available - Samples not available. Fetching samples...\n",
            "2025-06-01 22:01:59,935 - INFO - Evaluator - _get_samples - Fetching 21 samples from the test set.\n",
            "2025-06-01 22:01:59,936 - INFO - T5InferenceEngine - batch_correct - Processing batch 1 with size 16\n",
            "2025-06-01 22:02:01,000 - INFO - T5InferenceEngine - batch_correct - Processing batch 2 with size 5\n",
            "2025-06-01 22:02:01,805 - INFO - T5InferenceEngine - batch_correct - Batch correction completed. Total corrected sentences: 21\n",
            "2025-06-01 22:02:01,807 - INFO - Evaluator - evaluate_accuracy - Exact match accuracy on test set: 0.9048\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9047619047619048"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluate the finetuned model on the medical dataset\n",
        "medical_evaluator = Evaluator(\n",
        "    inference_engine=T5InferenceEngine(model_dir=medical_model_dir, max_length=650),\n",
        "    dataset=medical_split_dataset\n",
        ")\n",
        "medical_evaluator.evaluate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-06-01 22:02:13,434 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-06-01 22:02:13,436 - INFO - Evaluator - evaluate_gleu - Average GLEU score on test set: 0.9847\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9847041847041847"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "medical_evaluator.evaluate_gleu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is not way too much informative because there's only 21 samples in the test dataset but we can see improvement (i guess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLAMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We could use a larger model with more examples when doing prompt engineering, even fine tune it with all the medical data we have available. This won't be done in this notebook."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "055c7fcf598745cf99315e80e81709c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef8d3a0680b74c6e9df24794017ea12e",
            "placeholder": "​",
            "style": "IPY_MODEL_279659dc07444734b085c8169f867b70",
            "value": " 2191/2191 [00:01&lt;00:00, 1627.86 examples/s]"
          }
        },
        "08a6b7839b764f4fb776aea3af5acf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2297133da4464284ab9d40d2f6ced178",
              "IPY_MODEL_74498800f98e4fff8f6bb8071dc6d50d",
              "IPY_MODEL_72ef81b8e8a54eefa18b02b44f4874a3"
            ],
            "layout": "IPY_MODEL_fa5e46db179b4fe5a9da60eca0ebe021"
          }
        },
        "216b495b32934706b944cf39c4b22482": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2297133da4464284ab9d40d2f6ced178": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9bf949307354e12a70c3da2292058ed",
            "placeholder": "​",
            "style": "IPY_MODEL_48adf339642c40e09940e3c7365fb773",
            "value": "Map: 100%"
          }
        },
        "23e48d091d584996b6baf9d3534447cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "279659dc07444734b085c8169f867b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f5637f5aa8c4e6aadf6220867f5f6b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "315a9ddbc2284762b1e80bd0a92a0b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31ea86456fd14d2889100383253460b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b96becefb6e49af8b3413b929ba5cc0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e5f706edd4f40b89c51070623366d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e472c8b6ea8481a91ae73d55aa652b3",
              "IPY_MODEL_cd95e5d0f489478f94ecc09980f12e73",
              "IPY_MODEL_055c7fcf598745cf99315e80e81709c0"
            ],
            "layout": "IPY_MODEL_216b495b32934706b944cf39c4b22482"
          }
        },
        "3ec7a5e8f6b849f8bab515ee5b93af35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4195890956a7418da1a88f32677e7405": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43fa86bc7caf4bd1a4e06ace7e0b25b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45a9de88d3114a5e84c3838dfe93585f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48adf339642c40e09940e3c7365fb773": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e472c8b6ea8481a91ae73d55aa652b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b96becefb6e49af8b3413b929ba5cc0",
            "placeholder": "​",
            "style": "IPY_MODEL_315a9ddbc2284762b1e80bd0a92a0b4d",
            "value": "Map: 100%"
          }
        },
        "6c7ed26dff004f979e336abb19029e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23e48d091d584996b6baf9d3534447cd",
            "placeholder": "​",
            "style": "IPY_MODEL_31ea86456fd14d2889100383253460b1",
            "value": " 28350/28350 [00:20&lt;00:00, 863.56 examples/s]"
          }
        },
        "72ef81b8e8a54eefa18b02b44f4874a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdf4c595694342fd87d9b758ff52f087",
            "placeholder": "​",
            "style": "IPY_MODEL_e92402f5608045199acb7a0e8983f9c3",
            "value": " 2695/2695 [00:01&lt;00:00, 2193.87 examples/s]"
          }
        },
        "74498800f98e4fff8f6bb8071dc6d50d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f5637f5aa8c4e6aadf6220867f5f6b0",
            "max": 2695,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45a9de88d3114a5e84c3838dfe93585f",
            "value": 2695
          }
        },
        "899e8ba560184b1a8228a7df1b27c994": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97bc4ab3288e43448c1d4670156adee1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae683a19f8df4c5c89ecd2d1373d3c66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdc85929710b4c0a8bc404f845458970": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43fa86bc7caf4bd1a4e06ace7e0b25b2",
            "max": 28350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff69fbbae64a40218adf260529fdb5b4",
            "value": 28350
          }
        },
        "cd95e5d0f489478f94ecc09980f12e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae683a19f8df4c5c89ecd2d1373d3c66",
            "max": 2191,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ec7a5e8f6b849f8bab515ee5b93af35",
            "value": 2191
          }
        },
        "ce276d79bf73439c963e79b02c767794": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff40b7f85c3a4ec29b2493d6a96b8553",
              "IPY_MODEL_bdc85929710b4c0a8bc404f845458970",
              "IPY_MODEL_6c7ed26dff004f979e336abb19029e1f"
            ],
            "layout": "IPY_MODEL_4195890956a7418da1a88f32677e7405"
          }
        },
        "e92402f5608045199acb7a0e8983f9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef8d3a0680b74c6e9df24794017ea12e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9bf949307354e12a70c3da2292058ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa5e46db179b4fe5a9da60eca0ebe021": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdf4c595694342fd87d9b758ff52f087": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff40b7f85c3a4ec29b2493d6a96b8553": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97bc4ab3288e43448c1d4670156adee1",
            "placeholder": "​",
            "style": "IPY_MODEL_899e8ba560184b1a8228a7df1b27c994",
            "value": "Map: 100%"
          }
        },
        "ff69fbbae64a40218adf260529fdb5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
