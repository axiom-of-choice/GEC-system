{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Challenge related with a GEC system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Table of contents:\n",
        "\n",
        "- [Challenge related with a GEC system](#challenge-related-with-a-gec-system)\n",
        "- [Explanation](#explanation)\n",
        "- [Just Setting Up](#just-setting-up)\n",
        "- [Dataset Download and Loader](#dataset-downloader-and-loader)\n",
        "    - [Dataset Exploration and Parsing](#some-exploration-of-the-data-and-the-parsing)\n",
        "- [Preprocessing for T5 Model](#t5-preprocessor)\n",
        "- [Training the T5 Model](#trainer-class)\n",
        "- [Inference Engines](#inference-engines)\n",
        "    - [Base Inference Engine](#base-inference-engine-and-helper)\n",
        "    - [T5 Inference Engine](#t5-inference-engine)\n",
        "    - [Llama 3 Inference Engine](#llama-3-inference-engine)\n",
        "- [Evaluation](#evaluator-class)\n",
        "    - [T5 FCE evaluation](#evaluating-the-test-data-fce-with-t5)\n",
        "    - [Llama3 FCE evaluation](#evaluating-test-data-fce-with-llama-3)\n",
        "\n",
        "- [Final evaluation on medical data](#evaluating-our-medical-data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Just Setting Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfH9JRQoy-Nh"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers tqdm scikit-learn sentencepiece torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt1Eh9fX4G9z"
      },
      "outputs": [],
      "source": [
        "## Imports\n",
        "\n",
        "import os\n",
        "# import openai\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Tuple\n",
        "import os\n",
        "import tarfile\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "77f0SKp44CoF"
      },
      "outputs": [],
      "source": [
        "# Create logger\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Remove all handlers associated with the root logger object (avoid duplicate logs)\n",
        "for handler in logger.handlers[:]:\n",
        "    logger.removeHandler(handler)\n",
        "\n",
        "# Create handler that outputs to notebook cell\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(funcName)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "logger.addHandler(handler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "a03Tbnn20WGE"
      },
      "outputs": [],
      "source": [
        "# constants, ideally defined in other file and imported if done in a github repo\n",
        "\n",
        "FCE_URL = \"https://www.cl.cam.ac.uk/research/nl/bea2019st/data/fce_v2.1.bea19.tar.gz\"\n",
        "FCE_DEV_NAME = \"fce.dev.gold.bea19.m2\"\n",
        "FCE_TRAIN_NAME = \"fce.train.gold.bea19.m2\"\n",
        "FCE_TEST_NAME = \"fce.test.gold.bea19.m2\"\n",
        "FCE_DOWNLOAD_DATASET_DIR = os.path.join(os.getcwd(), \"data\")\n",
        "FCE_DATASET_DIR = os.path.join(os.getcwd(), \"data/fce/m2\")\n",
        "SENTENCE_TAG = \"S \"\n",
        "ANNOTATION_TAG = \"A \"\n",
        "NO_EDIT_TAG = \"noop\"\n",
        "SEP = \"|||\"\n",
        "## We're gonna use T5 small for this task because we're just correcting spelling and we don't wait to wait hours for training.\n",
        "MODEL_NAME = \"t5-small\"\n",
        "FINETUNED_MODEL_OUTPUT_DIR = \"./t5_finetuned\"\n",
        "PROMPT_PATH = os.path.join(os.getcwd(), \"config/prompt.txt\")\n",
        "LLAMA3_ENDPOINT = \"http://127.0.0.1:11434/api/generate\"\n",
        "TEXT_TO_REPLACE_IN_PROMPT = \"<text_to_replace>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Downloader and Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class for orchestrating the downloading and Transformers propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {
        "id": "er9tr-c83u1w"
      },
      "outputs": [],
      "source": [
        "### Dataset Loader class handling the dataset creation\n",
        "import tarfile\n",
        "from datasets import Dataset, DatasetDict\n",
        "import requests\n",
        "from typing import Optional, List, Dict\n",
        "\n",
        "\n",
        "class M2DatasetLoader:\n",
        "    \"\"\"\n",
        "    Loader for datasets in M2 format. Right now it's only tested for BCE2019\n",
        "\n",
        "    Args:\n",
        "        dataset_dir (str): Directory where the dataset is stored. Default is FCE_DATASET_DIR.\n",
        "        train_file (str): Name of the training file. Default is FCE_TRAIN_NAME.\n",
        "        dev_file (str): Name of the development file. Default is FCE_DEV_NAME.\n",
        "        test_file (str): Name of the test file. Default is FCE_TEST_NAME.\n",
        "        dataset_url (str): URL to download the dataset from. Default is FCE_URL.\n",
        "        fce_download_dir (str): Directory to download the dataset to. Default is FCE_DOWNLOAD_DATASET_DIR.\n",
        "    \"\"\"\n",
        "    def __init__(self,dataset_dir: str = FCE_DATASET_DIR, train_file: str = FCE_TRAIN_NAME,\n",
        "                 dev_file: str = FCE_DEV_NAME, test_file: str = FCE_TEST_NAME, dataset_url: str = FCE_URL,\n",
        "                 fce_download_dir: str = FCE_DOWNLOAD_DATASET_DIR):\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.fce_download_dir = fce_download_dir\n",
        "        self.train_file = os.path.join(dataset_dir, train_file)\n",
        "        self.dev_file = os.path.join(dataset_dir, dev_file)\n",
        "        self.test_file = os.path.join(dataset_dir, test_file)\n",
        "        self.url = dataset_url\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.dataset = None\n",
        "\n",
        "    def download_and_extract(self) -> None:\n",
        "        \"\"\"\n",
        "        Download the file and extract it into a directory if it does not exist\n",
        "        \"\"\"\n",
        "        if not os.path.exists(self.fce_download_dir):\n",
        "            os.makedirs(self.fce_download_dir, exist_ok=True)\n",
        "            self.logger.info(f\"Downloading BEA 2019 dataset...\")\n",
        "            tar_path = os.path.join(self.fce_download_dir, \"bea19.tar.gz\")\n",
        "            with requests.get(self.url, stream=True) as r:\n",
        "                with open(tar_path, 'wb') as f:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "            self.logger.info(\"Extracting dataset...\")\n",
        "            with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "                tar.extractall(path=self.fce_download_dir)\n",
        "            os.remove(tar_path)\n",
        "            self.logger.info(\"BEA dataset downloaded and extracted successfully.\")\n",
        "        else:\n",
        "            self.logger.warning(\"BEA dataset already exists locally.\")\n",
        "\n",
        "    def load_dataset(self) -> DatasetDict:\n",
        "        \"\"\" Create the dataset in the Transformers format \"\"\"\n",
        "        dataset = DatasetDict({\n",
        "            'train': Dataset.from_list(self._parse_m2_file(self.train_file)),\n",
        "            'validation': Dataset.from_list(self._parse_m2_file(self.dev_file)),\n",
        "            'test': Dataset.from_list(self._parse_m2_file(self.test_file))\n",
        "        })\n",
        "        self.logger.info(f\"Loaded BEA dataset: {len(dataset['train'])} train, {len(dataset['validation'])} dev, {len(dataset['test'])} test\")\n",
        "        self.dataset = dataset\n",
        "        return dataset\n",
        "    \n",
        "    def save_dataset(self, output_dir: str = os.path.join(os.getcwd(), FCE_DOWNLOAD_DATASET_DIR)) -> None:\n",
        "        \"\"\"\n",
        "        Parses the M2 files and saves the resulting DatasetDict to disk in HuggingFace format.\n",
        "        Args:\n",
        "            output_dir (str): Directory to save the dataset.\n",
        "        \"\"\"\n",
        "        if self.dataset is None:\n",
        "            self.dataset = self.load_dataset()\n",
        "        self.dataset.save_to_disk(output_dir + \"parsed_fce_dataset\")\n",
        "        self.logger.info(f\"Saved parsed dataset to {output_dir}\")\n",
        "\n",
        "    def _parse_m2_file(self, filepath: str) -> List[Dict[str, str]]:\n",
        "      \"\"\"\n",
        "      Parses an M2 file and returns a list of {source, target} dictionaries,\n",
        "      where each 'target' corresponds to one annotator's corrections.\n",
        "      \"\"\"\n",
        "      data = []\n",
        "      with open(filepath, 'r', encoding='utf-8') as f:\n",
        "          lines = f.readlines()\n",
        "\n",
        "      sentence = \"\"\n",
        "      edits_by_annotator = dict()\n",
        "      current_annotator = 0\n",
        "\n",
        "      for line in lines + ['\\n']:  # Add sentinel newline\n",
        "          line = line.strip()\n",
        "          if line.startswith(SENTENCE_TAG):\n",
        "              if sentence and edits_by_annotator:\n",
        "                  for annotator_id, edits in edits_by_annotator.items():\n",
        "                      corrected = self._apply_m2_edits(sentence, edits)\n",
        "                      data.append({'source': sentence, 'target': corrected})\n",
        "              sentence = line[2:]\n",
        "              edits_by_annotator = dict()\n",
        "          elif line.startswith(ANNOTATION_TAG):\n",
        "              parts = line[2:].split(SEP)\n",
        "              span = list(map(int, parts[0].split()))\n",
        "              error_type = parts[1]\n",
        "              correction = parts[2]\n",
        "              annotator_id = int(parts[-1])\n",
        "\n",
        "              # Initialize list of edits for this annotator\n",
        "              if annotator_id not in edits_by_annotator:\n",
        "                  edits_by_annotator[annotator_id] = []\n",
        "              edits_by_annotator[annotator_id].append((span, correction, error_type))\n",
        "          elif line == \"\" and sentence:\n",
        "              # End of current sentence block\n",
        "              if not edits_by_annotator:\n",
        "                  # No edits -> copy original sentence as is\n",
        "                  data.append({'source': sentence, 'target': sentence})\n",
        "              else:\n",
        "                  for annotator_id, edits in edits_by_annotator.items():\n",
        "                      corrected = self._apply_m2_edits(sentence, edits)\n",
        "                      data.append({'source': sentence, 'target': corrected})\n",
        "              sentence = \"\"\n",
        "              edits_by_annotator = dict()\n",
        "      return data\n",
        "\n",
        "\n",
        "    def _apply_m2_edits(self, sentence: str, edits:str):\n",
        "        \"\"\"\n",
        "        Applies M2 format edits to the original sentence.\n",
        "        :param sentence: Original sentence (string)\n",
        "        :param edits: List of (span, correction, error_type)\n",
        "        :return: Corrected sentence\n",
        "        \"\"\"\n",
        "        tokens = sentence.strip().split()\n",
        "        # Sort edits by start index descending to avoid offset issues\n",
        "        edits = sorted(edits, key=lambda x: x[0][0], reverse=True)\n",
        "\n",
        "        for (start, end), correction, error_type in edits:\n",
        "            if error_type == NO_EDIT_TAG:\n",
        "                continue\n",
        "            if start == -1 and end == -1:\n",
        "                continue  # skip noop or invalid spans\n",
        "            # Replace tokens[start:end] with correction tokens (split)\n",
        "            correction_tokens = correction.strip().split()\n",
        "            tokens = tokens[:start] + correction_tokens + tokens[end:]\n",
        "        return ' '.join(tokens)\n",
        "    \n",
        "    @staticmethod\n",
        "    def most_common_edit_types(gold_m2_path: str, n: int = 10) -> List[Tuple[str, int]]:\n",
        "        \"\"\"\n",
        "        Returns the most common edit types in the gold M2 file.\n",
        "        :param gold_m2_path: Path to the gold M2 file.\n",
        "        :param n: Number of most common edit types to return.\n",
        "        :return: List of tuples (edit_type, count).\n",
        "        \"\"\"\n",
        "        from collections import Counter\n",
        "        edit_types = Counter()\n",
        "        \n",
        "        with open(gold_m2_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.startswith(ANNOTATION_TAG):\n",
        "                    parts = line[2:].split(SEP)\n",
        "                    error_type = parts[1]\n",
        "                    edit_types[error_type] += 1\n",
        "        \n",
        "        return edit_types.most_common(n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86oVxDVb4V9G",
        "outputId": "5fff7ede-983a-4a29-bbb4-9b472e6b455c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 15:50:19,241 - WARNING - M2DatasetLoader - download_and_extract - BEA dataset already exists locally.\n",
            "2025-05-31 15:50:19,449 - INFO - M2DatasetLoader - load_dataset - Loaded BEA dataset: 28350 train, 2191 dev, 2695 test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving the dataset (1/1 shards): 100%|██████████| 28350/28350 [00:00<00:00, 3938020.15 examples/s]\n",
            "Saving the dataset (1/1 shards): 100%|██████████| 2191/2191 [00:00<00:00, 777472.09 examples/s]\n",
            "Saving the dataset (1/1 shards): 100%|██████████| 2695/2695 [00:00<00:00, 1329997.56 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 15:50:19,469 - INFO - M2DatasetLoader - save_dataset - Saved parsed dataset to /Users/isaac/Developer/GEC-system/data/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "loader = M2DatasetLoader()\n",
        "\n",
        "loader.download_and_extract()\n",
        "dataset = loader.load_dataset()\n",
        "loader.save_dataset(output_dir=os.getcwd() + \"/data/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some exploration of the data and the parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-e2ElvC3XC9"
      },
      "source": [
        "The file has one or more line per sentence, and one or more line per annotation, grouped together by a blank line.\n",
        "If there si a *noop* tag the there is no edit.\n",
        "\n",
        "\n",
        "We need a set of functions to actually put this corrections into the sentence to build up a dataset with \"source\" and \"target\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwkgL_Wj267x",
        "outputId": "a0f38808-8de6-45b8-e109-e674b679ed82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S Dear Sir or Madam ,\n",
            "A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S I am writing in order to express my disappointment about your musical show \" Over the Rainbow \" .\n",
            "A 9 10|||R:PREP|||with|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S I saws the show 's advertisement hanging up of a wall in London where I was spending my holiday with some friends . I convinced them to go there with me because I had heard good references about your Company and , above all , about the main star , Danny Brook .\n",
            "A 1 2|||R:VERB:TENSE|||saw|||REQUIRED|||-NONE-|||0\n",
            "A 8 9|||R:PREP|||on|||REQUIRED|||-NONE-|||0\n",
            "A 36 37|||R:NOUN|||reviews|||REQUIRED|||-NONE-|||0\n",
            "A 37 38|||R:PREP|||of|||REQUIRED|||-NONE-|||0\n",
            "A 45 46|||R:PREP|||because of|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S The problems started in the box office , where we asked for the discounts you announced in the advertisement , and the man who was selling the tickets said that they did n't exist .\n",
            "A 3 4|||R:PREP|||at|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S Moreover , the show was delayed forty - five minutes and the worst of all was that Danny Brook had been replaced by another actor .\n",
            "A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S On the other hand , the theatre restaurant was closed because unknown reasons .\n",
            "A 0 4|||R:OTHER|||In addition|||REQUIRED|||-NONE-|||0\n",
            "A 10 11|||R:PREP|||for|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S You promised a perfect evening but it became a big disastrous !\n",
            "A 10 11|||R:MORPH|||disaster|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S I would like some kind of explanation and receive my\n"
          ]
        }
      ],
      "source": [
        "# Need yo have the data downloaded to explore it. Will be automatized in the next class\n",
        "train_data = os.path.join(FCE_DATASET_DIR, FCE_TRAIN_NAME)\n",
        "\n",
        "\n",
        "with open(train_data, 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "print(data[:1500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3w3_zOY6O_M"
      },
      "source": [
        "To see what the parser is doing let's take two examples. One without edit and one with some edits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fHDKvJV6VUl"
      },
      "source": [
        "Without edits: (First record o the file actually)\n",
        "\n",
        "```S Dear Sir or Madam ,\n",
        "A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeGexy6k6laV",
        "outputId": "40239183-2598-4565-885f-9bb2f0f633c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrong\n",
            "Dear Sir or Madam ,\n",
            "Corrected\n",
            "Dear Sir or Madam ,\n"
          ]
        }
      ],
      "source": [
        "print(\"Wrong\")\n",
        "print(dataset['train'][0].get(\"source\"))\n",
        "print(\"Corrected\")\n",
        "print(dataset['train'][0].get(\"target\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3ZGWjXS8Fc9"
      },
      "source": [
        "Many edits, like third record of the file:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0QTaTBg8PeL"
      },
      "source": [
        "\"I saws the show 's advertisement hanging up of a wall in London where I was spending my holiday with some friends .\n",
        "I convinced them to go there with me because I had heard good references about your Company and , above all , about the main star , Danny Brook .\"\n",
        "\n",
        "```\n",
        "A 1 2|||R:VERB:TENSE|||saw|||REQUIRED|||-NONE-|||0\n",
        "A 8 9|||R:PREP|||on|||REQUIRED|||-NONE-|||0\n",
        "A 36 37|||R:NOUN|||reviews|||REQUIRED|||-NONE-|||0\n",
        "A 37 38|||R:PREP|||of|||REQUIRED|||-NONE-|||0\n",
        "A 45 46|||R:PREP|||because of|||REQUIRED|||-NONE-|||0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKwiifh28TY9"
      },
      "source": [
        "Explanation:\n",
        "\n",
        "It bascially changes *saws* by *saw*, then *of* by *on*, *references* by *reviews*, again *about* by *of* and finally *about* by *because of*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiOF8Cvk0EfO",
        "outputId": "b356f2ed-98af-4579-a216-516fdd6216c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrong\n",
            "I saws the show 's advertisement hanging up of a wall in London where I was spending my holiday with some friends . I convinced them to go there with me because I had heard good references about your Company and , above all , about the main star , Danny Brook .\n",
            "Corrected\n",
            "I saw the show 's advertisement hanging up on a wall in London where I was spending my holiday with some friends . I convinced them to go there with me because I had heard good reviews of your Company and , above all , because of the main star , Danny Brook .\n"
          ]
        }
      ],
      "source": [
        "print(\"Wrong\")\n",
        "print(dataset['train'][2].get(\"source\"))\n",
        "print(\"Corrected\")\n",
        "print(dataset['train'][2].get(\"target\"))\n",
        "#print(dataset['validation'][0])\n",
        "#print(dataset['test'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA8_NAwl9t6x"
      },
      "source": [
        "Lets see another example where we delete something (I looked for it in the file)\n",
        "\n",
        "*consequently* needs to get deleted\n",
        "\n",
        "```\n",
        "S If you do n't agree , I will act consequently .\n",
        "A 9 10|||U:ADV||||||REQUIRED|||-NONE-|||0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAV7qrVh9z4c",
        "outputId": "07a5f6c8-01f0-4c1a-adc4-8d492d76198b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrong\n",
            "If you do n't agree , I will act consequently .\n",
            "Corrected\n",
            "If you do n't agree , I will act .\n"
          ]
        }
      ],
      "source": [
        "print(\"Wrong\")\n",
        "print(dataset['train'][8].get(\"source\"))\n",
        "print(\"Corrected\")\n",
        "print(dataset['train'][8].get(\"target\"))\n",
        "#print(dataset['validation'][0])\n",
        "#print(dataset['test'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9j1fyo_-Opw"
      },
      "source": [
        "With punctuation\n",
        "\n",
        "In this case, we need to add a comma and a quoute\n",
        "\n",
        "```\n",
        "\n",
        "S She began to read \" Dear Carolin ..\n",
        "A 4 4|||M:PUNCT|||,|||REQUIRED|||-NONE-|||0\n",
        "A 8 8|||M:PUNCT|||\"|||REQUIRED|||-NONE-|||0\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7kORX_c-a5O",
        "outputId": "0a03d64d-1675-4fa3-b676-5178dfe95acc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrong\n",
            "She began to read \" Dear Carolin ..\n",
            "Corrected\n",
            "She began to read , \" Dear Carolin .. \"\n"
          ]
        }
      ],
      "source": [
        "print(\"Wrong\")\n",
        "print(dataset['train'][15].get(\"source\"))\n",
        "print(\"Corrected\")\n",
        "print(dataset['train'][15].get(\"target\"))\n",
        "#print(dataset['validation'][0])\n",
        "#print(dataset['test'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk2nkDWTADU7"
      },
      "source": [
        "Now that we already have a parser and loader of the dataset for our needs using M2 file, we will create a simple preprocessor for the T5 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T5 PreProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "Osxngv6n3X-z"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedTokenizer\n",
        "from datasets import DatasetDict\n",
        "from typing import Dict\n",
        "\n",
        "class T5Preprocessor:\n",
        "    \"\"\"\n",
        "    T5 Preprocessor for the data. Could be implemented as abstract class to have multiple preprocessors.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (PreTrainedTokenizer): The tokenizer to use.\n",
        "        max_lenght (int): The maximum lenght of the input. Default is 512.\n",
        "        truncation (bool): Whether to truncate the input. Default is True.\n",
        "        padding (str): The padding to use. Default is \"max_length\".\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, truncation: bool = True, padding: str = \"max_length\"):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.truncation = truncation\n",
        "        self.padding = padding\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    def _preprocess_function(self, examples: Dict[str,str], max_length: int) -> Dict[str, List[List[int]]]:\n",
        "        \"\"\"\n",
        "        Preprocess examples tokenizing them using the instance parameters.\n",
        "        \"\"\"\n",
        "        inputs = [\"correct grammar: \" + s for s in examples['source']]\n",
        "        targets = [t for t in examples['target']]\n",
        "        model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=self.truncation, padding=self.padding)\n",
        "        labels = self.tokenizer(targets, max_length=max_length, truncation=self.truncation, padding=self.padding)\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "    \n",
        "    def _get_max_input_length(self, dataset: DatasetDict) -> int:\n",
        "        \"\"\"\n",
        "        Get the maximum input length in the dataset.\n",
        "        \"\"\"\n",
        "        max_length = 0\n",
        "        for split in dataset:\n",
        "            split_max = max(len(x) for x in dataset[split]['source'])\n",
        "            self.logger.info(f\"Max input_ids length in {split}: {split_max}\")\n",
        "            max_length = max(max_length, split_max)\n",
        "            self.logger.info(f\"Overall max input_ids length: {max_length}\")\n",
        "        return max_length\n",
        "    \n",
        "    \n",
        "    def save_tokenized_dataset(self, dataset: DatasetDict, output_dir: str) -> None:\n",
        "        \"\"\"\n",
        "        Save the tokenized dataset to disk.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Saving tokenized dataset to {output_dir}...\")\n",
        "        dataset.save_to_disk(output_dir)\n",
        "        self.logger.info(\"Tokenized dataset saved successfully.\")\n",
        "\n",
        "    def preprocess(self, dataset: DatasetDict, max_length: int) -> DatasetDict:\n",
        "        \"\"\"\n",
        "        Preprocess the dataset.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Preprocessing dataset...\")\n",
        "        tokenized_dataset = dataset.map(lambda examples: self._preprocess_function(examples, max_length), batched=True)\n",
        "        self.logger.info(\"Dataset preprocessed\")\n",
        "        return tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147,
          "referenced_widgets": [
            "ce276d79bf73439c963e79b02c767794",
            "ff40b7f85c3a4ec29b2493d6a96b8553",
            "bdc85929710b4c0a8bc404f845458970",
            "6c7ed26dff004f979e336abb19029e1f",
            "4195890956a7418da1a88f32677e7405",
            "97bc4ab3288e43448c1d4670156adee1",
            "899e8ba560184b1a8228a7df1b27c994",
            "43fa86bc7caf4bd1a4e06ace7e0b25b2",
            "ff69fbbae64a40218adf260529fdb5b4",
            "23e48d091d584996b6baf9d3534447cd",
            "31ea86456fd14d2889100383253460b1",
            "3e5f706edd4f40b89c51070623366d55",
            "5e472c8b6ea8481a91ae73d55aa652b3",
            "cd95e5d0f489478f94ecc09980f12e73",
            "055c7fcf598745cf99315e80e81709c0",
            "216b495b32934706b944cf39c4b22482",
            "3b96becefb6e49af8b3413b929ba5cc0",
            "315a9ddbc2284762b1e80bd0a92a0b4d",
            "ae683a19f8df4c5c89ecd2d1373d3c66",
            "3ec7a5e8f6b849f8bab515ee5b93af35",
            "ef8d3a0680b74c6e9df24794017ea12e",
            "279659dc07444734b085c8169f867b70",
            "08a6b7839b764f4fb776aea3af5acf78",
            "2297133da4464284ab9d40d2f6ced178",
            "74498800f98e4fff8f6bb8071dc6d50d",
            "72ef81b8e8a54eefa18b02b44f4874a3",
            "fa5e46db179b4fe5a9da60eca0ebe021",
            "f9bf949307354e12a70c3da2292058ed",
            "48adf339642c40e09940e3c7365fb773",
            "2f5637f5aa8c4e6aadf6220867f5f6b0",
            "45a9de88d3114a5e84c3838dfe93585f",
            "fdf4c595694342fd87d9b758ff52f087",
            "e92402f5608045199acb7a0e8983f9c3"
          ]
        },
        "id": "Yoj7GoEAGFbh",
        "outputId": "2768d5be-3f00-4f2f-826c-da2f70a4ec46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 15:52:10,942 - INFO - T5Preprocessor - _get_max_input_length - Max input_ids length in train: 630\n",
            "2025-05-31 15:52:10,942 - INFO - T5Preprocessor - _get_max_input_length - Overall max input_ids length: 630\n",
            "2025-05-31 15:52:10,944 - INFO - T5Preprocessor - _get_max_input_length - Max input_ids length in validation: 510\n",
            "2025-05-31 15:52:10,944 - INFO - T5Preprocessor - _get_max_input_length - Overall max input_ids length: 630\n",
            "2025-05-31 15:52:10,946 - INFO - T5Preprocessor - _get_max_input_length - Max input_ids length in test: 454\n",
            "2025-05-31 15:52:10,946 - INFO - T5Preprocessor - _get_max_input_length - Overall max input_ids length: 630\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer\n",
        "from datasets import load_from_disk\n",
        "\n",
        "dataset = load_from_disk(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"parsed_fce_dataset\"))\n",
        "\n",
        "preprocessor = T5Preprocessor(tokenizer=T5Tokenizer.from_pretrained(MODEL_NAME))\n",
        "\n",
        "max_length = preprocessor._get_max_input_length(dataset)  # This will give you the max input length in the dataset for saving the preprocessed dataset to train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9zU9eBeGfO5",
        "outputId": "f120a7cc-60eb-4762-8903-6a7d629c7573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 21:46:04,981 - INFO - T5Preprocessor - preprocess - Preprocessing dataset...\n",
            "2025-05-31 21:46:05,050 - INFO - T5Preprocessor - preprocess - Dataset preprocessed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving the dataset (1/1 shards): 100%|██████████| 28350/28350 [00:00<00:00, 376752.99 examples/s]\n",
            "Saving the dataset (1/1 shards): 100%|██████████| 2191/2191 [00:00<00:00, 236379.35 examples/s]\n",
            "Saving the dataset (1/1 shards): 100%|██████████| 2695/2695 [00:00<00:00, 227213.60 examples/s]\n"
          ]
        }
      ],
      "source": [
        "preprocessed_dataset = preprocessor.preprocess(dataset, max_length=max_length)\n",
        "preprocessed_dataset.save_to_disk(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"preprocessed_fce_dataset\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFnnTmlWGAxT"
      },
      "source": [
        "Now what about the trainig class?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trainer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9ZPi0RWOGDej"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Optional\n",
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from datasets import DatasetDict\n",
        "import torch\n",
        "\n",
        "\n",
        "class T5Trainer:\n",
        "    def __init__(self, model_name: str = MODEL_NAME, output_dir: str = FINETUNED_MODEL_OUTPUT_DIR,\n",
        "                 logging_dir: str = \"./finetune_logs\", save_strategy: str = \"epoch\", resume_from_dir: Optional[str] = None,\n",
        "                 batch_size: int = 8, mixed_precision: bool = False, early_stopping: bool = True,\n",
        "                 eval_strategy: str = \"epoch\", early_stopping_patience: int = 3, early_stopping_threshold: float = 0.0, \n",
        "                 metric_for_best_model: str = \"eval_loss\", greater_is_better: bool = False):\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        self.output_dir = output_dir\n",
        "        self.logging_dir = logging_dir\n",
        "        self.save_strategy = save_strategy\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.resume_from_dir = resume_from_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.mixed_precision = mixed_precision\n",
        "        self.early_stopping = early_stopping\n",
        "        self.eval_strategy = eval_strategy\n",
        "        self.early_stopping_patience = early_stopping_patience\n",
        "        self.early_stopping_threshold = early_stopping_threshold\n",
        "        self.metric_for_best_model = metric_for_best_model\n",
        "        self.greater_is_better = greater_is_better\n",
        "        \n",
        "\n",
        "    def _check_if_gpu_available(self) -> bool:\n",
        "        \"\"\"\n",
        "        Check if GPU is available.\n",
        "        \"\"\"\n",
        "        available = torch.cuda.is_available()\n",
        "\n",
        "        if available:\n",
        "            self.logger.info(\"GPU available\")\n",
        "        else:\n",
        "            self.logger.warning(\"GPU not available\")\n",
        "\n",
        "        return available\n",
        "\n",
        "    def _check_if_model_already_available(self) -> bool:\n",
        "        \"\"\"\n",
        "        Check if model is already available.\n",
        "        \"\"\"\n",
        "        available = os.path.exists(self.output_dir)\n",
        "\n",
        "        if available:\n",
        "            self.logger.info(\"Model already available\")\n",
        "        else:\n",
        "            self.logger.warning(\"Model not available\")\n",
        "\n",
        "        return available\n",
        "\n",
        "    def _create_unique_dir_for_model(self) -> str:\n",
        "\n",
        "        \"\"\"\n",
        "        Create a unique directory for the model based on the current time.\n",
        "        \"\"\"\n",
        "        current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        unique_dir = os.path.join(self.output_dir, current_time)\n",
        "        os.makedirs(unique_dir, exist_ok=True)\n",
        "        self.logger.info(f\"Created unique directory for model: {unique_dir}\")\n",
        "        return unique_dir\n",
        "\n",
        "\n",
        "    def train(self, tokenized_dataset:DatasetDict, epochs=3, learning_rate: float = 3e-4, eval_strategy = \"epoch\") -> Tuple[str, T5ForConditionalGeneration, T5Tokenizer]:\n",
        "        \"\"\"\n",
        "        Train the T5 model.\n",
        "        Args:\n",
        "            tokenized_dataset (DatasetDict): The tokenized dataset.\n",
        "            epochs (int): The number of epochs to train for.\n",
        "        Returns:\n",
        "            Tuple[Model_Dir (ID), T5ForConditionalGeneration, T5Tokenizer]: The trained model and tokenizer.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Checking if GPU is available...\")\n",
        "        self._check_if_gpu_available()\n",
        "        #self.logger.info(\"Checking if model is already available...\")\n",
        "        #self._check_if_model_already_available()\n",
        "        # Determine the output directory based on whether we're resuming or starting fresh\n",
        "        if self.resume_from_dir and os.path.exists(self.resume_from_dir):\n",
        "            self.output_dir = self.resume_from_dir\n",
        "            self.logger.info(f\"Resuming training from directory: {self.output_dir}\")\n",
        "        else:\n",
        "             self.output_dir = self._create_unique_dir_for_model()\n",
        "             self.logger.info(f\"Starting new training in directory: {self.output_dir}\")\n",
        "\n",
        "        # self.output_dir = self._create_unique_dir_for_model()\n",
        "\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.output_dir,\n",
        "            per_device_train_batch_size=self.batch_size,\n",
        "            num_train_epochs=epochs,\n",
        "            eval_strategy = self.eval_strategy,\n",
        "            save_strategy=self.save_strategy,\n",
        "            logging_dir=self.logging_dir,\n",
        "            learning_rate=learning_rate,\n",
        "            report_to=\"none\", #Needed to avoid wandb api key request.,\n",
        "            # fp16=self.mixed_precision,  # Enable mixed precision training if specified\n",
        "        )\n",
        "        \n",
        "        if self.early_stopping:\n",
        "            training_args.load_best_model_at_end = True\n",
        "            training_args.metric_for_best_model = self.metric_for_best_model\n",
        "            training_args.greater_is_better = self.greater_is_better\n",
        "            training_args.save_total_limit = 1\n",
        "            training_args.eval_strategy = self.eval_strategy\n",
        "        self.logger.info(\"Training arguments set\")\n",
        "        \n",
        "        if self.mixed_precision:\n",
        "            training_args.fp16 = True\n",
        "            self.logger.info(\"Mixed precision training enabled\")\n",
        "\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset['train'],\n",
        "            eval_dataset=tokenized_dataset['validation']\n",
        "        )\n",
        "        \n",
        "        if self.early_stopping:\n",
        "            trainer.add_callback(EarlyStoppingCallback(self.early_stopping_patience, self.early_stopping_threshold))\n",
        "            self.logger.info(\"Early stopping callback added\")\n",
        "\n",
        "          # Resume training if a checkpoint exists in the output directory\n",
        "        if self.resume_from_dir and os.path.exists(self.resume_from_dir):\n",
        "             # The Trainer class automatically handles resuming from a directory if it exists\n",
        "             # and contains a checkpoint. You don't need to explicitly load the state dicts\n",
        "             # if you are using the Trainer's resume functionality.\n",
        "             trainer.train(resume_from_checkpoint=self.output_dir)\n",
        "        else:\n",
        "            trainer.train()\n",
        "\n",
        "\n",
        "        self.logger.info(\"T5 model trained\")\n",
        "        trainer.save_model(self.output_dir)\n",
        "        self.model.save_pretrained(self.output_dir)\n",
        "        self.tokenizer.save_pretrained(self.output_dir)\n",
        "        return self.output_dir, self.model, self.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_empty_finetuned_model_dirs(output_dir: str = FINETUNED_MODEL_OUTPUT_DIR) -> None:\n",
        "    \"\"\"\n",
        "    Clean empty directories in the output directory.\n",
        "    Args:\n",
        "        output_dir (str): The output directory to clean.\n",
        "    \"\"\"\n",
        "    # Only check immediate children, do not traverse deeper\n",
        "    for name in os.listdir(output_dir):\n",
        "        path = os.path.join(output_dir, name)\n",
        "        if os.path.isdir(path) and not os.listdir(path):\n",
        "            os.rmdir(path)\n",
        "            logger.info(f\"Removed empty directory: {path}\")\n",
        "        else:\n",
        "            logger.info(f\"Directory {path} is not empty or not a directory, skipping.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clean_empty_finetuned_model_dirs(os.getcwd() + \"/models/t5_finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "preprocessed_dataset = load_from_disk(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"preprocessed_fce_dataset\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "E0OHnY7bHgwl",
        "outputId": "2585a7c2-5200-4e4c-ed1d-c8bc54733194"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#trainer = T5Trainer(resume_from_dir=\"./t5_finetuned/20250530-172532\")\n",
        "# trainer = T5Trainer(batch_size=16, resume_from_dir=\"/Users/isaac/Developer/sample/t5_finetuned/20250530-121239/checkpoint-591\")  # Enable mixed precision training\n",
        "\n",
        "trainer = T5Trainer(batch_size=8)\n",
        "\n",
        "model_dir, model, tokenizer = trainer.train(preprocessed_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference Engines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base Inference Engine and Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "class BaseInferenceEngine(ABC):\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "    @abstractmethod\n",
        "    def correct_sentence(self, sentence: str) -> str:\n",
        "        pass\n",
        "    @abstractmethod\n",
        "    def batch_correct(self, sentences: list, batch_size: int = 16) -> list:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## T5 Inference Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, Union, Optional\n",
        "\n",
        "\n",
        "class T5InferenceEngine(BaseInferenceEngine):\n",
        "    def __init__(self, model_dir: str, max_length: Optional[int] = None):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.t5_model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
        "        self.max_length = max_length if max_length is not None else self.tokenizer.model_max_length\n",
        "        self.logger.info(f\"T5 model loaded from {model_dir} with max length {self.max_length}\")\n",
        "        \n",
        "\n",
        "    def correct_sentence(self, sentence:str) -> str:\n",
        "        \"\"\"\n",
        "        Corrects a sentence using the T5 model.\n",
        "        Args:\n",
        "            sentence (str): The sentence to correct.\n",
        "        Returns:\n",
        "            str: The corrected sentence.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Correcting sentence: {sentence}\")\n",
        "        inputs = self.tokenizer(\"correct grammar: \" + sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length)\n",
        "        outputs = self.t5_model.generate(**inputs, max_length=self.max_length)\n",
        "        corrected_sentence = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        self.logger.info(f\"Corrected sentence: {corrected_sentence}\")\n",
        "        return corrected_sentence\n",
        "    \n",
        "    def batch_correct(self, sentences: list, batch_size: int = 16) -> list:\n",
        "        \"\"\"\n",
        "        Corrects a batch of sentences using the T5 model.\n",
        "        Args:\n",
        "            sentences (list): List of sentences to correct.\n",
        "            batch_size (int): Number of sentences per batch.\n",
        "        Returns:\n",
        "            list: List of corrected sentences.\n",
        "        \"\"\"\n",
        "        corrected = []\n",
        "        for i in range(0, len(sentences), batch_size):\n",
        "            self.logger.info(f\"Processing batch {i // batch_size + 1} with size {min(batch_size, len(sentences) - i)}\")\n",
        "            batch = sentences[i:i+batch_size]\n",
        "            inputs = self.tokenizer(\n",
        "                [\"correct grammar: \" + s for s in batch],\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=self.max_length\n",
        "            )\n",
        "            outputs = self.t5_model.generate(**inputs, max_length=self.max_length)\n",
        "            batch_corrected = [self.tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
        "            corrected.extend(batch_corrected)\n",
        "        self.logger.info(f\"Batch correction completed. Total corrected sentences: {len(corrected)}\")\n",
        "        return corrected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 09:56:54,110 - INFO - T5InferenceEngine - __init__ - T5 model loaded from /Users/isaac/Developer/GEC-system/models/finished with max length 650\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "650"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dir = \"/Users/isaac/Developer/GEC-system/models/finished\"\n",
        "t5_inference_engine = T5InferenceEngine(model_dir=model_dir, max_length=650)\n",
        "t5_inference_engine.max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 09:57:14,637 - INFO - T5InferenceEngine - correct_sentence - Correcting sentence: You is a apple.\n",
            "2025-05-31 09:57:14,715 - INFO - T5InferenceEngine - correct_sentence - Corrected sentence: You are an apple.\n",
            "Wrong sentence: You is a apple.\n",
            "Corrected sentence: You are an apple.\n"
          ]
        }
      ],
      "source": [
        "wrong_sentence = \"You is a apple.\"\n",
        "theorycally_corrected_sentence = \"I have an apple.\"\n",
        "\n",
        "\n",
        "corrected_sentence = t5_inference_engine.correct_sentence(wrong_sentence)\n",
        "\n",
        "print(f\"Wrong sentence: {wrong_sentence}\")\n",
        "print(f\"Corrected sentence: {corrected_sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/isaac/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#%pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Llama 3 Inference Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See docs to run and query llama3 8b locally using ollama\n",
        "\n",
        "https://ollama.com/library/llama3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Union, Dict\n",
        "import json as _json\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import requests\n",
        "\n",
        "class Llama3InferenceEngine(BaseInferenceEngine):\n",
        "    model_name: str = \"llama3\"\n",
        "    stream: bool = False\n",
        "    response_format: dict = {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"original_text\": {\"type\": \"string\"},\n",
        "            \"corrected_text\": {\"type\": \"string\"}\n",
        "        },\n",
        "        \"required\": [\"original_text\", \"corrected_text\"]\n",
        "    }\n",
        "    \n",
        "    options: Dict[str, Union[str, int, float]] = {\n",
        "        \"temperature\": 0.0,\n",
        "        \"seed\": 123,\n",
        "        \"top_k\": 10,\n",
        "        \"top_p\": 0.5\n",
        "    }\n",
        "    \n",
        "    \n",
        "    def __init__(self, model_endpoint: str, prompt_path: str):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.model_endpoint = model_endpoint\n",
        "        self.prompt_path = prompt_path\n",
        "        self.prompt = self._parse_prompt()\n",
        "        self.logger.info(f\"Llama3InferenceEngine initialized with model endpoint: {self.model_endpoint} and prompt path: {self.prompt_path}, options: {self.options}\")\n",
        "        \n",
        "    def _parse_prompt(self) -> str:\n",
        "        with open(self.prompt_path, 'r') as file:\n",
        "            prompt = file.read()\n",
        "        if not prompt:\n",
        "            self.logger.error(\"Prompt is empty. Please check the prompt file.\")\n",
        "            raise ValueError(\"Prompt is empty. Please check the prompt file.\")\n",
        "        return prompt\n",
        "    \n",
        "    def _replace_prompt_variables(self, sentence: str) -> str:\n",
        "        return self.prompt.replace(f\"{TEXT_TO_REPLACE_IN_PROMPT}\", sentence)\n",
        "\n",
        "    def send_correct_request(self, sentence: str) -> Dict[str, Union[str, Dict[str, str]]]:\n",
        "        prompt = self._replace_prompt_variables(sentence)\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                self.model_endpoint, \n",
        "                json={\n",
        "                    \"model\": self.model_name,\n",
        "                    \"prompt\": str(prompt),\n",
        "                    \"stream\": self.stream,\n",
        "                    \"format\": self.response_format,\n",
        "                    \"options\": self.options\n",
        "                }\n",
        "            )\n",
        "            response_data = response.json()\n",
        "            if not response_data:\n",
        "                self.logger.error(\"No corrected sentence returned from the model.\")\n",
        "                raise ValueError(\"No corrected sentence returned from the model.\")\n",
        "            if \"response\" in response_data and isinstance(response_data[\"response\"], str):\n",
        "                response_data[\"response\"] = _json.loads(response_data[\"response\"])\n",
        "            return response_data\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            self.logger.error(f\"Error during model inference: {e}\")\n",
        "            raise RuntimeError(f\"Error during model inference: {e}\")\n",
        "        \n",
        "    def correct_sentence(self, sentence: str) -> str:\n",
        "        self.logger.info(f\"Correcting sentence: {sentence}\")\n",
        "        response = self.send_correct_request(sentence)\n",
        "        if isinstance(response, dict):\n",
        "            corrected_sentence = response.get(\"response\", {}).get(\"corrected_text\", \"\")\n",
        "            self.logger.info(f\"Corrected sentence: {corrected_sentence}\")\n",
        "            return corrected_sentence\n",
        "        else:\n",
        "            return \"\"\n",
        "        \n",
        "    def batch_correct(self, sentences: list, batch_size: int = 16) -> list:\n",
        "        \"\"\"Not implemented for Llama3InferenceEngine as it does not support full batch inference yet.\n",
        "        This method is a placeholder to maintain interface consistency with BaseInferenceEngine.\n",
        "\n",
        "        Args:\n",
        "            sentences (list): _description_\n",
        "            batch_size (int, optional): _description_. Defaults to 16.\n",
        "\n",
        "        Returns:\n",
        "            list: _description_\n",
        "        \"\"\"\n",
        "        return super().batch_correct(sentences, batch_size)\n",
        "\n",
        "    # --- ASYNC BATCH INFERENCE ---\n",
        "    async def async_correct_sentence(self, session, sentence: str) -> str:\n",
        "        prompt = self._replace_prompt_variables(sentence)\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"prompt\": str(prompt),\n",
        "            \"stream\": self.stream,\n",
        "            \"format\": self.response_format,\n",
        "            \"options\": self.options\n",
        "        }\n",
        "        self.logger.info(f\"Sending async request for sentence: {sentence}\")\n",
        "        async with session.post(self.model_endpoint, json=payload) as resp:\n",
        "            response_data = await resp.json()\n",
        "            if not response_data:\n",
        "                return \"\"\n",
        "            if \"response\" in response_data and isinstance(response_data[\"response\"], str):\n",
        "                import json as _json\n",
        "                response_data[\"response\"] = _json.loads(response_data[\"response\"])\n",
        "                corrected_text = response_data.get(\"response\", {}).get(\"corrected_text\", \"\")\n",
        "                self.logger.info(f\"Async corrected sentence: {corrected_text}\")\n",
        "            \n",
        "                return corrected_text if corrected_text else \"\"\n",
        "            else:\n",
        "                self.logger.error(\"Response format is not as expected.\")\n",
        "                return \"\"\n",
        "\n",
        "    async def async_batch_correct(self, sentences, max_concurrent=5):\n",
        "        timeout = aiohttp.ClientTimeout(total=60)\n",
        "        semaphore = asyncio.Semaphore(max_concurrent)\n",
        "        async with aiohttp.ClientSession(timeout=timeout) as session:\n",
        "            async def sem_task(sentence):\n",
        "                async with semaphore:\n",
        "                    return await self.async_correct_sentence(session, sentence)\n",
        "            tasks = [sem_task(s) for s in sentences]\n",
        "            return await asyncio.gather(*tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "llama3_engine = Llama3InferenceEngine(model_endpoint=LLAMA3_ENDPOINT, prompt_path=PROMPT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model': 'llama3',\n",
              " 'created_at': '2025-05-31T17:36:30.449223Z',\n",
              " 'response': {'original_text': 'Helo', 'corrected_text': 'Hello'},\n",
              " 'done': True,\n",
              " 'done_reason': 'stop',\n",
              " 'context': [128006,\n",
              "  882,\n",
              "  128007,\n",
              "  271,\n",
              "  2675,\n",
              "  527,\n",
              "  459,\n",
              "  6335,\n",
              "  6498,\n",
              "  11311,\n",
              "  11397,\n",
              "  13,\n",
              "  4718,\n",
              "  3465,\n",
              "  374,\n",
              "  311,\n",
              "  4495,\n",
              "  904,\n",
              "  69225,\n",
              "  62172,\n",
              "  11,\n",
              "  43529,\n",
              "  11,\n",
              "  477,\n",
              "  62603,\n",
              "  6103,\n",
              "  304,\n",
              "  279,\n",
              "  2768,\n",
              "  11914,\n",
              "  13,\n",
              "  3234,\n",
              "  539,\n",
              "  2349,\n",
              "  279,\n",
              "  7438,\n",
              "  477,\n",
              "  1742,\n",
              "  315,\n",
              "  279,\n",
              "  11914,\n",
              "  13,\n",
              "  8442,\n",
              "  471,\n",
              "  279,\n",
              "  37065,\n",
              "  11914,\n",
              "  11,\n",
              "  449,\n",
              "  912,\n",
              "  5217,\n",
              "  16540,\n",
              "  382,\n",
              "  85664,\n",
              "  25,\n",
              "  16183,\n",
              "  78,\n",
              "  128009,\n",
              "  128006,\n",
              "  78191,\n",
              "  128007,\n",
              "  271,\n",
              "  90,\n",
              "  330,\n",
              "  10090,\n",
              "  4424,\n",
              "  794,\n",
              "  330,\n",
              "  39,\n",
              "  20782,\n",
              "  498,\n",
              "  330,\n",
              "  20523,\n",
              "  291,\n",
              "  4424,\n",
              "  794,\n",
              "  330,\n",
              "  9906,\n",
              "  1,\n",
              "  335],\n",
              " 'total_duration': 4884461166,\n",
              " 'load_duration': 10360125,\n",
              " 'prompt_eval_count': 63,\n",
              " 'prompt_eval_duration': 165838875,\n",
              " 'eval_count': 19,\n",
              " 'eval_duration': 893736917}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llama3_engine.send_correct_request(\"Helo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 00:01:11,833 - INFO - Llama3InferenceEngine - correct_sentence - Correcting sentence: Helo\n",
            "2025-05-31 00:01:12,732 - INFO - Llama3InferenceEngine - correct_sentence - Corrected sentence: Hello\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Hello'"
            ]
          },
          "execution_count": 215,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llama3_engine.correct_sentence(\"Helo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.translate.gleu_score import sentence_gleu\n",
        "from typing import Optional\n",
        "import re\n",
        "\n",
        "class Evaluator:\n",
        "    \"\"\" Evaluator class for evaluating the performance of the inference engine on a dataset.\n",
        "    Args:\n",
        "        inference_engine (BaseInferenceEngine): The inference engine to use for evaluation.\n",
        "        n_samples (Optional[int]): Number of samples to evaluate. If None, evaluates the entire test set. Default is None.\n",
        "        predicted_dataset (Optional[DatasetDict]): A precomputed dataset with predictions. If provided, it will use this dataset instead of running inference. Default is None.\n",
        "        dataset (DatasetDict): The dataset to evaluate. It also runs inference.\n",
        "    \"\"\"\n",
        "    def __init__(self, inference_engine: BaseInferenceEngine, n_samples: Optional[int] = None, predicted_dataset: Optional[DatasetDict] = None, dataset: Optional[DatasetDict] = None):\n",
        "        # self.dataset = dataset\n",
        "        self.engine = inference_engine\n",
        "        self.n_samples = n_samples\n",
        "        self.test_data, self.references, self.predictions = None, None, None\n",
        "        if dataset is None and predicted_dataset is None:\n",
        "            raise ValueError(\"Either dataset or predicted_dataset must be provided.\")\n",
        "        if predicted_dataset is not None and isinstance(predicted_dataset, DatasetDict):\n",
        "            self.test_data = predicted_dataset['test']['source']\n",
        "            self.references = predicted_dataset['test']['target']\n",
        "            self.predictions = predicted_dataset['test']['prediction']\n",
        "        elif dataset is not None and isinstance(dataset, DatasetDict):\n",
        "            self.dataset = dataset\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_text(text):\n",
        "        # Remove spaces before punctuation and ensure single space after\n",
        "        text = re.sub(r'\\s+([.,!?;:\"])', r'\\1', text)\n",
        "        text = re.sub(r'([.,!?;:\"])([^\\s])', r'\\1 \\2', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _get_samples(self) -> Tuple[List[str], List[str], List[str]]:\n",
        "        \"\"\" Fetch samples from the test set.\"\"\"\n",
        "        test_data = self.dataset['test'] if self.test_data is None else self.dataset['test']\n",
        "        n = min(self.n_samples, len(test_data)) if (self.n_samples is not None and self.n_samples > 0) else len(test_data)\n",
        "        self.logger.info(f\"Fetching {n} samples from the test set.\")\n",
        "        sample_sentences = [test_data[i]['source'] for i in range(n)]\n",
        "        references = [test_data[i]['target'] for i in range(n)]\n",
        "        predictions = self.engine.batch_correct(sample_sentences)\n",
        "        return sample_sentences, references, predictions\n",
        "    \n",
        "    def  _get_samples_if_not_available(self):\n",
        "        \"\"\"\n",
        "        Check if samples are available.\n",
        "        \"\"\"\n",
        "        if self.test_data is None or self.references is None or self.predictions is None:\n",
        "            self.logger.warning(\"Samples not available. Fetching samples...\")\n",
        "            self.test_data, self.references, self.predictions = self._get_samples()\n",
        "        else:\n",
        "            self.logger.info(\"Samples already available. Using cached samples.\")\n",
        "\n",
        "    def evaluate_accuracy(self) -> float:\n",
        "        \"\"\" \n",
        "        Evaluate the accuracy of the predictions against the references.\n",
        "        It computes the exact match accuracy.\n",
        "        Returns:\n",
        "            float: The exact match accuracy.\n",
        "        \"\"\"\n",
        "        self._get_samples_if_not_available()\n",
        "        norm_refs = [self.normalize_text(str(ref)) for ref in self.references]\n",
        "        norm_preds = [self.normalize_text(str(pred)) for pred in self.predictions]\n",
        "        accuracy = accuracy_score(norm_refs, norm_preds)\n",
        "        self.logger.info(f\"Exact match accuracy on test set: {accuracy:.4f}\")\n",
        "        return accuracy\n",
        "\n",
        "    def evaluate_gleu(self):\n",
        "        \"\"\"\n",
        "        Evaluate the GLEU score of the predictions against the references.\n",
        "        It uses the nltk library to compute the GLEU score.\n",
        "        Returns:\n",
        "            float: The average GLEU score across all samples.\n",
        "            \n",
        "        \"\"\"\n",
        "        self._get_samples_if_not_available()\n",
        "        gleu_scores = []\n",
        "        for pred, ref in zip(self.predictions, self.references):\n",
        "            ref_tokens = self.normalize_text(str(ref)).split()\n",
        "            pred_tokens = self.normalize_text(str(pred)).split()\n",
        "            gleu = sentence_gleu([ref_tokens], pred_tokens)\n",
        "            gleu_scores.append(gleu)\n",
        "        avg_gleu = sum(gleu_scores) / len(gleu_scores) if gleu_scores else 0.0\n",
        "        self.logger.info(f\"Average GLEU score on test set: {avg_gleu:.4f}\")\n",
        "        return avg_gleu\n",
        "\n",
        "    # --- ASYNC BATCH INFERENCE FOR LLAMA3 ---\n",
        "    async def evaluate_accuracy_async(self)-> Optional[float]:\n",
        "        \"\"\" Evaluate the accuracy of the predictions against the references using async batch inference.\n",
        "        It computes the exact match accuracy.\n",
        "        This method is specifically designed for engines that support async batch inference.\n",
        "\n",
        "        Returns:\n",
        "            Optional[float]: The exact match accuracy if async batch inference is available, otherwise None.\n",
        "        \"\"\"\n",
        "        # Fetch samples (sentences and references)\n",
        "        test_data = self.dataset['test']\n",
        "        n = min(self.n_samples, len(test_data)) if (self.n_samples is not None and self.n_samples > 0) else len(test_data)\n",
        "        self.logger.info(f\"Fetching {n} samples from the test set (async).\")\n",
        "        sample_sentences = [test_data[i]['source'] for i in range(n)]\n",
        "        references = [test_data[i]['target'] for i in range(n)]\n",
        "        # Use async batch correct if available\n",
        "        if hasattr(self.engine, \"async_batch_correct\"):\n",
        "            predictions = await self.engine.async_batch_correct(sample_sentences)\n",
        "            self.test_data, self.references, self.predictions = sample_sentences, references, predictions\n",
        "            norm_refs = [self.normalize_text(ref) for ref in self.references]\n",
        "            norm_preds = [self.normalize_text(pred) for pred in self.predictions]\n",
        "            accuracy = accuracy_score(norm_refs, norm_preds)\n",
        "            self.logger.info(f\"Exact match accuracy on test set: {accuracy:.4f}\")\n",
        "            return accuracy\n",
        "        else:\n",
        "            self.logger.warning(\"Async batch inference not available for this engine.\")\n",
        "            return None\n",
        "\n",
        "    async def evaluate_gleu_async(self)-> Optional[float]:\n",
        "        \"\"\" Evaluate the GLEU score of the predictions against the references using async batch inference.\n",
        "        It uses the nltk library to compute the GLEU score.\n",
        "        This method is specifically designed for engines that support async batch inference.\n",
        "\n",
        "        Returns:\n",
        "            Optional[float]: The average GLEU score across all samples if async batch inference is available, otherwise None.\n",
        "        \"\"\"\n",
        "        # Fetch samples (sentences and references)\n",
        "        test_data = self.dataset['test']\n",
        "        n = min(self.n_samples, len(test_data)) if (self.n_samples is not None and self.n_samples > 0) else len(test_data)\n",
        "        self.logger.info(f\"Fetching {n} samples from the test set (async).\")\n",
        "        sample_sentences = [test_data[i]['source'] for i in range(n)]\n",
        "        references = [test_data[i]['target'] for i in range(n)]\n",
        "        # Use async batch correct if available\n",
        "        if hasattr(self.engine, \"async_batch_correct\"):\n",
        "            predictions = await self.engine.async_batch_correct(sample_sentences)\n",
        "            self.test_data, self.references, self.predictions = sample_sentences, references, predictions\n",
        "            gleu_scores = []\n",
        "            for pred, ref in zip(self.predictions, self.references):\n",
        "                ref_tokens = self.normalize_text(ref).split()\n",
        "                pred_tokens = self.normalize_text(pred).split()\n",
        "                gleu = sentence_gleu([ref_tokens], pred_tokens)\n",
        "                gleu_scores.append(gleu)\n",
        "            avg_gleu = sum(gleu_scores) / len(gleu_scores) if gleu_scores else 0.0\n",
        "            self.logger.info(f\"Average GLEU score on test set: {avg_gleu:.4f}\")\n",
        "            return avg_gleu\n",
        "        else:\n",
        "            self.logger.warning(\"Async batch inference not available for this engine.\")\n",
        "            return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating the test dataset (FCE with T5) using GLEU and exact match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {},
      "outputs": [],
      "source": [
        "## This dataset was generated using the following command:\n",
        "# python3 -m scripts.predictions_builder medical t5 fce /Users/isaac/Developer/GEC-system/data/fce_predicted.csv\n",
        "\n",
        "import pandas as pd\n",
        "predicted_fce_df = pd.read_csv(os.path.join(\"/Users/isaac/Developer/GEC-system/data/fce_predicted.csv\"))\n",
        "predicted_fce_dataset = DatasetDict({\n",
        "    'test': Dataset.from_pandas(predicted_df)\n",
        "})\n",
        "\n",
        "\n",
        "# If you don't want to use it, comment the previous lines and uncomment the following \n",
        "# preprocessed_dataset = load_from_disk(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"preprocessed_fce_dataset\"))\n",
        "# from datasets import load_from_disk\n",
        "# preprocessed_dataset = load_from_disk(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"preprocessed_fce_dataset\"))\n",
        "# t5_inference_engine = T5InferenceEngine(model_dir=model_dir, max_length=650)\n",
        "\n",
        "# t5_evaluator= Evaluator(\n",
        "#     inference_engine=t5_inference_engine,\n",
        "#     dataset=preprocessed_dataset\n",
        "# )\n",
        "# accuracy = t5_evaluator.evaluate_accuracy()\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "# gleu = t5_evaluator.evaluate_gleu()\n",
        "# print(\"GLEU:\", gleu)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "t5_fce_evaluator = Evaluator(t5_inference_engine, predicted_dataset=predicted_fce_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 22:13:19,079 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-05-31 22:13:19,135 - INFO - Evaluator - evaluate_accuracy - Exact match accuracy on test set: 0.3800\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.3799628942486085"
            ]
          },
          "execution_count": 284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t5_fce_evaluator.evaluate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 22:13:19,922 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-05-31 22:13:20,057 - INFO - Evaluator - evaluate_gleu - Average GLEU score on test set: 0.7836\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7835986906022476"
            ]
          },
          "execution_count": 285,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t5_fce_evaluator.evaluate_gleu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating test data (FCE with Llama 3) using GLEU and exact match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 23:26:19,144 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-05-31 23:26:19,194 - INFO - Evaluator - evaluate_accuracy - Exact match accuracy on test set: 0.1325\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.13246753246753246"
            ]
          },
          "execution_count": 310,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_fce_llama_df = pd.read_csv(\"/Users/isaac/Developer/GEC-system/data/fce_predicted_llama.csv\")\n",
        "\n",
        "llama3_fce_evaluator = Evaluator(llama3_engine, predicted_dataset=DatasetDict({'test': Dataset.from_pandas(predicted_fce_llama_df)}))\n",
        "llama3_fce_evaluator.evaluate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 23:26:20,804 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-05-31 23:26:20,946 - INFO - Evaluator - evaluate_gleu - Average GLEU score on test set: 0.6072\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.6072433371173874"
            ]
          },
          "execution_count": 311,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llama3_fce_evaluator.evaluate_gleu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating our medical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we want to evaluate with our medical data to see how it behaves..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['source', 'target'],\n",
              "    num_rows: 204\n",
              "})"
            ]
          },
          "execution_count": 272,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Read your medical data CSV (update the path as needed)\n",
        "medical_df = pd.read_csv(\"/Users/isaac/Developer/GEC-system/data/data.csv\")  # columns should be ['source', 'target']\n",
        "medical_df.rename(columns={'incorrect_sentence': 'source', 'correct_sentence': 'target'}, inplace=True)\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "medical_data = Dataset.from_pandas(medical_df)\n",
        "medical_dataset_dict = DatasetDict({'test': medical_data})\n",
        "\n",
        "\n",
        "medical_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating with T5 using exact match and GLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Precomputed predictions for the medical dataset\n",
        "\n",
        "medical_t5_predictions_df = pd.read_csv(\"/Users/isaac/Developer/GEC-system/data/medical_predicted.csv\")\n",
        "\n",
        "t5_medical_evaluator = Evaluator(t5_inference_engine, predicted_dataset=DatasetDict({'test': Dataset.from_pandas(medical_t5_predictions_df)}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 22:14:28,078 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-05-31 22:14:28,084 - INFO - Evaluator - evaluate_accuracy - Exact match accuracy on test set: 0.1324\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.1323529411764706"
            ]
          },
          "execution_count": 289,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t5_medical_evaluator.evaluate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 22:14:50,386 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-05-31 22:14:50,407 - INFO - Evaluator - evaluate_gleu - Average GLEU score on test set: 0.7898\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7898051106345906"
            ]
          },
          "execution_count": 291,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t5_medical_evaluator.evaluate_gleu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating with Llama3 using exact match and GLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 22:15:20,596 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 22:15:20,601 - INFO - Evaluator - evaluate_accuracy - Exact match accuracy on test set: 0.1765\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.17647058823529413"
            ]
          },
          "execution_count": 292,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Precomputed predictions for the medical dataset using Llama3\n",
        "\n",
        "llama3_medical_predictions_df = pd.read_csv(\"/Users/isaac/Developer/GEC-system/data/medical_predicted_llama.csv\")\n",
        "\n",
        "llama3_medical_evaluator = Evaluator(llama3_engine, predicted_dataset=DatasetDict({'test': Dataset.from_pandas(llama3_medical_predictions_df)}))\n",
        "llama3_medical_evaluator.evaluate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-31 22:15:24,934 - INFO - Evaluator - _get_samples_if_not_available - Samples already available. Using cached samples.\n",
            "2025-05-31 22:15:24,949 - INFO - Evaluator - evaluate_gleu - Average GLEU score on test set: 0.7359\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.7359494647205326"
            ]
          },
          "execution_count": 293,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llama3_medical_evaluator.evaluate_gleu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using ERRANT Scorer on test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate gold FCE data using our framework \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate gold FCE data using our framework \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(os.path.join(os.getcwd(), \"data\", \"fce_predicted.csv\"))[\"target\"].tolist()\n",
        "\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce_gold.txt\"), 'w') as f:\n",
        "    for sentence in data:\n",
        "        f.write(Evaluator.normalize_text(sentence) + \"\\n\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ]
        }
      ],
      "source": [
        "!errant_parallel -orig ./data/fce_wrong.txt -cor ./data/fce_gold.txt -out ./data/m2/fce_gold.m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data is already predicted and stored as csv in data/fce_predicted.csv. Now we're gonna convert the predictions as txt as expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read your medical data CSV (update the path as needed)\n",
        "fce_predicted_df_t5 = pd.read_csv(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce_predicted.csv\"))  # columns should be ['source', 'target']\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce_predicted_t5.txt\"), 'w') as f:\n",
        "    for sentence in fce_predicted_df_t5['prediction']:\n",
        "        f.write(Evaluator.normalize_text(sentence) + '\\n')\n",
        "\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce_wrong.txt\"), 'w') as f:\n",
        "    for sentence in fce_predicted_df_t5['source']:\n",
        "        f.write(Evaluator.normalize_text(sentence) + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate system M2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install --upgrade --force-reinstall numpy h5py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ]
        }
      ],
      "source": [
        "!errant_parallel -orig ./data/fce_wrong.txt -cor ./data/fce_predicted_t5.txt -out ./data/m2/fce_predicted_t5.m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run it as script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "1307\t1323\t3353\t0.497\t0.2805\t0.4305\n",
            "==============================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!errant_compare -hyp ./data/m2/fce_predicted_t5.m2 -ref ./data/m2/fce_gold.m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "F0.5 = 0.43 is a solid result for a grammatical error correction (GEC) system, especially if you are using a small or moderately sized model, or if this is your first or baseline system.\n",
        "\n",
        "Precision (0.497) is much higher than recall (0.2805), meaning your system is conservative: it makes fewer edits, but they are more likely to be correct.\n",
        "\n",
        "Recall could be improved (your system is missing some errors), but this is common for many GEC systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLAMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running analysis over the most common edits in the medical data so we can finetune the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('noop', 903),\n",
              " ('R:OTHER', 573),\n",
              " ('R:NOUN', 553),\n",
              " ('R:SPELL', 403),\n",
              " ('M:DET', 307),\n",
              " ('R:ORTH', 251),\n",
              " ('R:PREP', 249),\n",
              " ('R:VERB', 233),\n",
              " ('R:VERB:TENSE', 164),\n",
              " ('U:DET', 133)]"
            ]
          },
          "execution_count": 321,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M2DatasetLoader.most_common_edit_types(gold_m2_path=\"./data/m2/fce_gold.m2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {},
      "outputs": [],
      "source": [
        "### LLAMA\n",
        "import pandas as pd\n",
        "\n",
        "# Read your medical data CSV (update the path as needed)\n",
        "fce_predicted_df_llama = pd.read_csv(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce_predicted_llama.csv\"))  # columns should be ['source', 'target']\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"fce_predicted_llama.txt\"), 'w') as f:\n",
        "    for sentence in fce_predicted_df_llama['prediction']:\n",
        "        f.write(Evaluator.normalize_text(str(sentence)) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ]
        }
      ],
      "source": [
        "!errant_parallel -orig ./data/fce_wrong.txt -cor ./data/fce_predicted_llama.txt -out ./data/m2/fce_predicted_llama.m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "1753\t5093\t2907\t0.2561\t0.3762\t0.2735\n",
            "==============================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!errant_compare -hyp ./data/m2/fce_predicted_llama.m2 -ref ./data/m2/fce_gold.m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is making some mistakes due to some lack of context and training. Still baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Errant Score on medical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate m2 files including gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "medical_data = pd.read_csv(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_predicted.csv\"))\n",
        "\n",
        "\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_wrong.txt\"), 'w') as f:\n",
        "    for sentence in medical_data['source']:\n",
        "        f.write(Evaluator.normalize_text(sentence) + '\\n')\n",
        "        \n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_gold.txt\"), 'w') as f:\n",
        "    for sentence in medical_data['target']:\n",
        "        f.write(sentence + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ]
        }
      ],
      "source": [
        "!errant_parallel -orig ./data/medical_wrong.txt -cor ./data/medical_gold.txt -out ./data/m2/medical_gold.m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read your medical data CSV (update the path as needed)\n",
        "medical_predicted_df_t5 = pd.read_csv(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_predicted.csv\"))  # columns should be ['source', 'target']\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_predicted_t5.txt\"), 'w') as f:\n",
        "    for sentence in medical_predicted_df_t5['prediction']:\n",
        "        f.write(Evaluator.normalize_text(sentence) + '\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ]
        }
      ],
      "source": [
        "!errant_parallel -orig ./data/medical_wrong.txt -cor ./data/medical_predicted_t5.txt -out ./data/m2/medical_predicted_t5.m2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "31\t184\t208\t0.1442\t0.1297\t0.141\n",
            "==============================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!errant_compare -hyp ./data/m2/medical_predicted_t5.m2 -ref ./data/m2/medical_gold.m2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLAMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running analysis over the most common edits in the medical data so we can finetune the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('R:VERB:TENSE', 177),\n",
              " ('M:DET', 36),\n",
              " ('R:VERB:FORM', 7),\n",
              " ('R:VERB:SVA', 6),\n",
              " ('R:PREP', 5),\n",
              " ('M:OTHER', 1),\n",
              " ('R:OTHER', 1),\n",
              " ('U:OTHER', 1),\n",
              " ('M:VERB:TENSE', 1),\n",
              " ('R:ADV', 1)]"
            ]
          },
          "execution_count": 332,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M2DatasetLoader.most_common_edit_types(gold_m2_path=\"./data/m2/medical_gold.m2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read your medical data CSV (update the path as needed)\n",
        "medical_predicted_df_llama = pd.read_csv(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_predicted_llama.csv\"))  # columns should be ['source', 'target']\n",
        "with open(os.path.join(FCE_DOWNLOAD_DATASET_DIR, \"medical_predicted_llama.txt\"), 'w') as f:\n",
        "    for sentence in medical_predicted_df_llama['prediction']:\n",
        "        f.write(Evaluator.normalize_text(str(sentence)) + '\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading resources...\n",
            "Processing parallel files...\n"
          ]
        }
      ],
      "source": [
        "!errant_parallel -orig ./data/medical_wrong.txt -cor ./data/medical_predicted_llama.txt -out ./data/m2/medical_predicted_llama.m2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('R:VERB:SVA', 123),\n",
              " ('M:DET', 88),\n",
              " ('R:VERB:TENSE', 39),\n",
              " ('R:NOUN', 27),\n",
              " ('R:NOUN:NUM', 26),\n",
              " ('R:OTHER', 15),\n",
              " ('R:SPELL', 9),\n",
              " ('M:OTHER', 8),\n",
              " ('R:MORPH', 7),\n",
              " ('R:VERB:FORM', 5)]"
            ]
          },
          "execution_count": 343,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M2DatasetLoader.most_common_edit_types(gold_m2_path=\"./data/m2/medical_predicted_llama.m2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 344,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "76\t292\t163\t0.2065\t0.318\t0.2221\n",
            "==============================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!errant_compare -hyp ./data/m2/medical_predicted_llama.m2 -ref ./data/m2/medical_gold.m2"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "055c7fcf598745cf99315e80e81709c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef8d3a0680b74c6e9df24794017ea12e",
            "placeholder": "​",
            "style": "IPY_MODEL_279659dc07444734b085c8169f867b70",
            "value": " 2191/2191 [00:01&lt;00:00, 1627.86 examples/s]"
          }
        },
        "08a6b7839b764f4fb776aea3af5acf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2297133da4464284ab9d40d2f6ced178",
              "IPY_MODEL_74498800f98e4fff8f6bb8071dc6d50d",
              "IPY_MODEL_72ef81b8e8a54eefa18b02b44f4874a3"
            ],
            "layout": "IPY_MODEL_fa5e46db179b4fe5a9da60eca0ebe021"
          }
        },
        "216b495b32934706b944cf39c4b22482": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2297133da4464284ab9d40d2f6ced178": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9bf949307354e12a70c3da2292058ed",
            "placeholder": "​",
            "style": "IPY_MODEL_48adf339642c40e09940e3c7365fb773",
            "value": "Map: 100%"
          }
        },
        "23e48d091d584996b6baf9d3534447cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "279659dc07444734b085c8169f867b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f5637f5aa8c4e6aadf6220867f5f6b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "315a9ddbc2284762b1e80bd0a92a0b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31ea86456fd14d2889100383253460b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b96becefb6e49af8b3413b929ba5cc0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e5f706edd4f40b89c51070623366d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e472c8b6ea8481a91ae73d55aa652b3",
              "IPY_MODEL_cd95e5d0f489478f94ecc09980f12e73",
              "IPY_MODEL_055c7fcf598745cf99315e80e81709c0"
            ],
            "layout": "IPY_MODEL_216b495b32934706b944cf39c4b22482"
          }
        },
        "3ec7a5e8f6b849f8bab515ee5b93af35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4195890956a7418da1a88f32677e7405": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43fa86bc7caf4bd1a4e06ace7e0b25b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45a9de88d3114a5e84c3838dfe93585f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48adf339642c40e09940e3c7365fb773": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e472c8b6ea8481a91ae73d55aa652b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b96becefb6e49af8b3413b929ba5cc0",
            "placeholder": "​",
            "style": "IPY_MODEL_315a9ddbc2284762b1e80bd0a92a0b4d",
            "value": "Map: 100%"
          }
        },
        "6c7ed26dff004f979e336abb19029e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23e48d091d584996b6baf9d3534447cd",
            "placeholder": "​",
            "style": "IPY_MODEL_31ea86456fd14d2889100383253460b1",
            "value": " 28350/28350 [00:20&lt;00:00, 863.56 examples/s]"
          }
        },
        "72ef81b8e8a54eefa18b02b44f4874a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdf4c595694342fd87d9b758ff52f087",
            "placeholder": "​",
            "style": "IPY_MODEL_e92402f5608045199acb7a0e8983f9c3",
            "value": " 2695/2695 [00:01&lt;00:00, 2193.87 examples/s]"
          }
        },
        "74498800f98e4fff8f6bb8071dc6d50d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f5637f5aa8c4e6aadf6220867f5f6b0",
            "max": 2695,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45a9de88d3114a5e84c3838dfe93585f",
            "value": 2695
          }
        },
        "899e8ba560184b1a8228a7df1b27c994": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97bc4ab3288e43448c1d4670156adee1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae683a19f8df4c5c89ecd2d1373d3c66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdc85929710b4c0a8bc404f845458970": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43fa86bc7caf4bd1a4e06ace7e0b25b2",
            "max": 28350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff69fbbae64a40218adf260529fdb5b4",
            "value": 28350
          }
        },
        "cd95e5d0f489478f94ecc09980f12e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae683a19f8df4c5c89ecd2d1373d3c66",
            "max": 2191,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ec7a5e8f6b849f8bab515ee5b93af35",
            "value": 2191
          }
        },
        "ce276d79bf73439c963e79b02c767794": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff40b7f85c3a4ec29b2493d6a96b8553",
              "IPY_MODEL_bdc85929710b4c0a8bc404f845458970",
              "IPY_MODEL_6c7ed26dff004f979e336abb19029e1f"
            ],
            "layout": "IPY_MODEL_4195890956a7418da1a88f32677e7405"
          }
        },
        "e92402f5608045199acb7a0e8983f9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef8d3a0680b74c6e9df24794017ea12e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9bf949307354e12a70c3da2292058ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa5e46db179b4fe5a9da60eca0ebe021": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdf4c595694342fd87d9b758ff52f087": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff40b7f85c3a4ec29b2493d6a96b8553": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97bc4ab3288e43448c1d4670156adee1",
            "placeholder": "​",
            "style": "IPY_MODEL_899e8ba560184b1a8228a7df1b27c994",
            "value": "Map: 100%"
          }
        },
        "ff69fbbae64a40218adf260529fdb5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
